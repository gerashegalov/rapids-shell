{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4146aabb-1c55-415e-b60c-23a1dcc1184a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Start jupyter-lab\n",
    "\n",
    "```bash\n",
    "jupyter-lab --notebook-dir=$HOME/gits/gerashegalov/rapids-shell/src/jupyter\n",
    "```\n",
    "or simply open in VS Code with Jupyter extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9c9ac",
   "metadata": {},
   "source": [
    "# Repro for [TIMESTAMP_MICROS missing logicalType](https://github.com/NVIDIA/spark-rapids/issues/8778) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4010da20-e354-4b10-b3ce-000344c59daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy\n",
    "import fastparquet\n",
    "import findspark\n",
    "import glob\n",
    "import os\n",
    "import pandas\n",
    "import sys\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72923f92",
   "metadata": {},
   "source": [
    "### Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152499e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_version = 'cuda11'\n",
    "hadoop_version = '3'\n",
    "rapids_version = '23.06.0'\n",
    "scala_version = '2.12'\n",
    "spark_version = '3.3.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84d10a",
   "metadata": {},
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1b741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'\n",
    "os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'\n",
    "os.environ['TZ'] = 'UTC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab907e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.environ['HOME']\n",
    "work_dir = f\"{home_dir}/jupyter_run_dir\"\n",
    "m2_local_repo = f\"{home_dir}/.m2/repository\"\n",
    "groupId = \"com.nvidia\"\n",
    "artifactId = f\"rapids-4-spark_{scala_version}\"\n",
    "dist_jar = f\"{artifactId}-{rapids_version}-{cuda_version}.jar\"\n",
    "spark_home = f\"{home_dir}/dist/spark-{spark_version}-bin-hadoop{hadoop_version}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29534b21",
   "metadata": {},
   "source": [
    "### Find Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1dfd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(spark_home = spark_home)\n",
    "findspark.add_jars(f\"{m2_local_repo}/com/nvidia/{artifactId}/{rapids_version}/{dist_jar}\")\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242319ae",
   "metadata": {},
   "source": [
    "### Configure Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15dd6b39-9f91-4855-ab13-25989937df4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for transport dt_socket at address: 5005\n",
      "23/07/25 22:43:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/25 22:43:09 WARN RapidsPluginUtils: RAPIDS Accelerator 23.06.0 using cudf 23.06.0.\n",
      "23/07/25 22:43:09 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n",
      "23/07/25 22:43:09 WARN RapidsPluginUtils: spark.rapids.sql.explain is set to `ALL`. Set it to 'NONE' to suppress the diagnostics logging about the query placement on the GPU.\n"
     ]
    }
   ],
   "source": [
    "cores_per_exec = 1\n",
    "jdwp = '-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005'\n",
    "spark_master = f\"local[{cores_per_exec}]\"\n",
    "spark_builder = pyspark.sql.SparkSession.builder\n",
    "spark_builder.config('spark.app.name', 'RAPIDS PySpark Notebook')\n",
    "spark_builder.config('spark.driver.extraJavaOptions', f\"-Dai.rapids.cudf.preserve-dependencies=true {jdwp}\")\n",
    "spark_builder.config('spark.master', spark_master)\n",
    "spark_builder.config('spark.plugins', 'com.nvidia.spark.SQLPlugin')\n",
    "spark_builder.config('spark.rapids.sql.enabled', True)\n",
    "spark_builder.config('spark.rapids.sql.explain', 'ALL')\n",
    "\n",
    "spark = spark_builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31634b93",
   "metadata": {},
   "source": [
    "# Test Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2438c36-df9b-4177-85da-b114a0d16f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(datetime.datetime(3023, 7, 14, 7, 38, 45, 418688),)], 'ts timestamp')\n",
    "cpu_path = tempfile.mkdtemp(\"cpu_ts\")\n",
    "gpu_path = tempfile.mkdtemp(\"gpu_ts\")\n",
    "spark.conf.set('spark.sql.parquet.outputTimestampType', 'TIMESTAMP_MICROS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf8a18ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_to_fastparquet(on_gpu = False):\n",
    "    spark.conf.set('spark.rapids.sql.enabled', on_gpu)\n",
    "    path = gpu_path if on_gpu else cpu_path\n",
    "    df.write.mode('overwrite').parquet(path)\n",
    "    spark.read.parquet(path).show(truncate = False)\n",
    "    file_path, = glob.glob(f\"{path}/*.parquet\")\n",
    "    fastparquet_file = fastparquet.ParquetFile(file_path)\n",
    "    print(fastparquet_file.head(1))\n",
    "    print(fastparquet_file.fmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ddfb5",
   "metadata": {},
   "source": [
    "Read from CPU Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ee003ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|ts                        |\n",
      "+--------------------------+\n",
      "|3023-07-14 07:38:45.418688|\n",
      "+--------------------------+\n",
      "\n",
      "                          ts\n",
      "0 3023-07-14 07:38:45.418688\n",
      "column_orders:\n",
      "- TYPE_ORDER: {}\n",
      "created_by: b'parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)'\n",
      "encryption_algorithm: null\n",
      "footer_signing_key_metadata: null\n",
      "key_value_metadata:\n",
      "- key: b'org.apache.spark.version'\n",
      "  value: b'3.3.2'\n",
      "- key: b'org.apache.spark.sql.parquet.row.metadata'\n",
      "  value: b'{\"type\":\"struct\",\"fields\":[{\"name\":\"ts\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}]}'\n",
      "num_rows: 1\n",
      "row_groups:\n",
      "- columns:\n",
      "  - column_index_length: 31\n",
      "    column_index_offset: 43\n",
      "    crypto_metadata: null\n",
      "    encrypted_column_metadata: null\n",
      "    file_offset: 4\n",
      "    file_path: null\n",
      "    meta_data:\n",
      "      bloom_filter_offset: null\n",
      "      codec: 1\n",
      "      data_page_offset: 4\n",
      "      dictionary_page_offset: null\n",
      "      encoding_stats:\n",
      "      - count: 1\n",
      "        encoding: 0\n",
      "        page_type: 0\n",
      "      encodings:\n",
      "      - 4\n",
      "      - 0\n",
      "      - 3\n",
      "      index_page_offset: null\n",
      "      key_value_metadata: null\n",
      "      num_values: 1\n",
      "      path_in_schema:\n",
      "      - ts\n",
      "      statistics:\n",
      "        distinct_count: null\n",
      "        max: b'\\xc0N$\\xedD\\x1dv\\x00'\n",
      "        max_value: b'\\xc0N$\\xedD\\x1dv\\x00'\n",
      "        min: b'\\xc0N$\\xedD\\x1dv\\x00'\n",
      "        min_value: b'\\xc0N$\\xedD\\x1dv\\x00'\n",
      "        null_count: 0\n",
      "      total_compressed_size: 39\n",
      "      total_uncompressed_size: 37\n",
      "      type: 2\n",
      "    offset_index_length: 10\n",
      "    offset_index_offset: 74\n",
      "  file_offset: 4\n",
      "  num_rows: 1\n",
      "  ordinal: 0\n",
      "  sorting_columns: null\n",
      "  total_byte_size: 37\n",
      "  total_compressed_size: 39\n",
      "schema:\n",
      "- converted_type: null\n",
      "  field_id: null\n",
      "  logicalType: null\n",
      "  name: spark_schema\n",
      "  num_children: 1\n",
      "  precision: null\n",
      "  repetition_type: null\n",
      "  scale: null\n",
      "  type: null\n",
      "  type_length: null\n",
      "- converted_type: 10\n",
      "  field_id: null\n",
      "  logicalType:\n",
      "    BSON: null\n",
      "    DATE: null\n",
      "    DECIMAL: null\n",
      "    ENUM: null\n",
      "    INTEGER: null\n",
      "    JSON: null\n",
      "    LIST: null\n",
      "    MAP: null\n",
      "    STRING: null\n",
      "    TIME: null\n",
      "    TIMESTAMP:\n",
      "      isAdjustedToUTC: true\n",
      "      unit:\n",
      "        MICROS: {}\n",
      "        MILLIS: null\n",
      "        NANOS: null\n",
      "    UNKNOWN: null\n",
      "    UUID: null\n",
      "  name: ts\n",
      "  num_children: null\n",
      "  precision: null\n",
      "  repetition_type: 1\n",
      "  scale: null\n",
      "  type: 2\n",
      "  type_length: null\n",
      "version: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_to_fastparquet(on_gpu = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0280e5f0",
   "metadata": {},
   "source": [
    "Read from GPU Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a0f39ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/25 22:43:24 WARN GpuOverrides: \n",
      "*Exec <DataWritingCommandExec> will run on GPU\n",
      "  *Output <InsertIntoHadoopFsRelationCommand> will run on GPU\n",
      "  ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "    @Expression <AttributeReference> ts#0 could run on GPU\n",
      "\n",
      "23/07/25 22:43:25 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  *Exec <ProjectExec> will run on GPU\n",
      "    *Expression <Alias> cast(ts#12 as string) AS ts#15 will run on GPU\n",
      "      *Expression <Cast> cast(ts#12 as string) will run on GPU\n",
      "    *Exec <FileSourceScanExec> will run on GPU\n",
      "\n",
      "+--------------------------+\n",
      "|ts                        |\n",
      "+--------------------------+\n",
      "|3023-07-14 07:38:45.418688|\n",
      "+--------------------------+\n",
      "\n",
      "                             ts\n",
      "0 1854-06-04 08:29:37.999584768\n",
      "column_orders:\n",
      "- TYPE_ORDER: {}\n",
      "created_by: null\n",
      "encryption_algorithm: null\n",
      "footer_signing_key_metadata: null\n",
      "key_value_metadata:\n",
      "- key: b'org.apache.spark.sql.parquet.row.metadata'\n",
      "  value: b'{\"type\":\"struct\",\"fields\":[{\"name\":\"ts\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}]}'\n",
      "- key: b'org.apache.spark.version'\n",
      "  value: b'3.3.2'\n",
      "num_rows: 1\n",
      "row_groups:\n",
      "- columns:\n",
      "  - column_index_length: null\n",
      "    column_index_offset: null\n",
      "    crypto_metadata: null\n",
      "    encrypted_column_metadata: null\n",
      "    file_offset: 0\n",
      "    file_path: null\n",
      "    meta_data:\n",
      "      bloom_filter_offset: null\n",
      "      codec: 0\n",
      "      data_page_offset: 4\n",
      "      dictionary_page_offset: null\n",
      "      encoding_stats: null\n",
      "      encodings:\n",
      "      - 0\n",
      "      - 3\n",
      "      index_page_offset: null\n",
      "      key_value_metadata: null\n",
      "      num_values: 1\n",
      "      path_in_schema:\n",
      "      - ts\n",
      "      statistics:\n",
      "        distinct_count: null\n",
      "        max: null\n",
      "        max_value: b'\\xc0N$\\xedD\\x1dv\\x00'\n",
      "        min: null\n",
      "        min_value: b'\\xc0N$\\xedD\\x1dv\\x00'\n",
      "        null_count: 0\n",
      "      total_compressed_size: 31\n",
      "      total_uncompressed_size: 31\n",
      "      type: 2\n",
      "    offset_index_length: null\n",
      "    offset_index_offset: null\n",
      "  file_offset: null\n",
      "  num_rows: 1\n",
      "  ordinal: null\n",
      "  sorting_columns: null\n",
      "  total_byte_size: 31\n",
      "  total_compressed_size: null\n",
      "schema:\n",
      "- converted_type: null\n",
      "  field_id: null\n",
      "  logicalType: null\n",
      "  name: schema\n",
      "  num_children: 1\n",
      "  precision: null\n",
      "  repetition_type: null\n",
      "  scale: null\n",
      "  type: null\n",
      "  type_length: null\n",
      "- converted_type: 10\n",
      "  field_id: null\n",
      "  logicalType: null\n",
      "  name: ts\n",
      "  num_children: null\n",
      "  precision: null\n",
      "  repetition_type: 1\n",
      "  scale: null\n",
      "  type: 2\n",
      "  type_length: null\n",
      "version: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_to_fastparquet(on_gpu = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7c7d9",
   "metadata": {},
   "source": [
    "# Python Package Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c470b252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name=fastparquet version=2023.7.0\n",
      "\n",
      "name=numpy version=1.25.1\n",
      "\n",
      "name=pandas version=2.0.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in [fastparquet, numpy, pandas]:\n",
    "    print(f\"name={p.__name__} version={p.__version__}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "612f3343",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package           Version\n",
      "----------------- --------\n",
      "asttokens         2.2.1\n",
      "backcall          0.2.0\n",
      "cfgv              3.3.1\n",
      "comm              0.1.3\n",
      "cramjam           2.6.2\n",
      "debugpy           1.6.7\n",
      "decorator         5.1.1\n",
      "distlib           0.3.7\n",
      "exceptiongroup    1.1.2\n",
      "execnet           2.0.2\n",
      "executing         1.2.0\n",
      "fastparquet       2023.7.0\n",
      "filelock          3.12.2\n",
      "findspark         2.0.1\n",
      "fsspec            2023.6.0\n",
      "identify          2.5.25\n",
      "iniconfig         2.0.0\n",
      "ipykernel         6.24.0\n",
      "ipython           8.14.0\n",
      "jedi              0.18.2\n",
      "jupyter_client    8.3.0\n",
      "jupyter_core      5.3.1\n",
      "matplotlib-inline 0.1.6\n",
      "nest-asyncio      1.5.6\n",
      "nodeenv           1.8.0\n",
      "numpy             1.25.1\n",
      "packaging         23.1\n",
      "pandas            2.0.3\n",
      "parso             0.8.3\n",
      "pexpect           4.8.0\n",
      "pickleshare       0.7.5\n",
      "pip               22.0.2\n",
      "platformdirs      3.9.1\n",
      "pluggy            1.2.0\n",
      "pre-commit        3.3.3\n",
      "prompt-toolkit    3.0.39\n",
      "psutil            5.9.5\n",
      "ptyprocess        0.7.0\n",
      "pure-eval         0.2.2\n",
      "pyarrow           12.0.1\n",
      "Pygments          2.15.1\n",
      "pytest            7.4.0\n",
      "pytest-xdist      3.3.1\n",
      "python-dateutil   2.8.2\n",
      "pytz              2023.3\n",
      "PyYAML            6.0.1\n",
      "pyzmq             25.1.0\n",
      "setuptools        59.6.0\n",
      "six               1.16.0\n",
      "sre-yield         1.2\n",
      "stack-data        0.6.2\n",
      "tomli             2.0.1\n",
      "tornado           6.3.2\n",
      "traitlets         5.9.0\n",
      "tzdata            2023.3\n",
      "virtualenv        20.24.1\n",
      "wcwidth           0.2.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "24e1e440b58174d23459f334e6373b3f84a303d378405919ee47af328df355be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
