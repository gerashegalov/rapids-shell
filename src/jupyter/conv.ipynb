{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4146aabb-1c55-415e-b60c-23a1dcc1184a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Start jupyter-lab\n",
    "\n",
    "```bash\n",
    "jupyter-lab --notebook-dir=$HOME/gits/gerashegalov/rapids-shell/src/jupyter\n",
    "```\n",
    "or simply open in VS Code with Jupyter extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9c9ac",
   "metadata": {},
   "source": [
    "# Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4010da20-e354-4b10-b3ce-000344c59daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72923f92",
   "metadata": {},
   "source": [
    "### Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152499e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_version = 'cuda11'\n",
    "hadoop_version = '3'\n",
    "java_version = '8'\n",
    "rapids_version = '23.10.0-SNAPSHOT'\n",
    "scala_version = '2.12'\n",
    "spark_version = '3.3.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84d10a",
   "metadata": {},
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1b741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME'] = f\"/usr/lib/jvm/java-{java_version}-openjdk-amd64\"\n",
    "os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'\n",
    "os.environ['TZ'] = 'UTC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab907e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.environ['HOME']\n",
    "work_dir = f\"{home_dir}/jupyter_run_dir\"\n",
    "m2_local_repo = f\"{home_dir}/.m2/repository\"\n",
    "groupId = \"com.nvidia\"\n",
    "artifactId = f\"rapids-4-spark_{scala_version}\"\n",
    "dist_jar = f\"{artifactId}-{rapids_version}-{cuda_version}.jar\"\n",
    "spark_home = f\"{home_dir}/dist/spark-{spark_version}-bin-hadoop{hadoop_version}\"\n",
    "# spark_home = f\"{home_dir}/gits/apache/spark\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29534b21",
   "metadata": {},
   "source": [
    "### Find Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1dfd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(spark_home = spark_home)\n",
    "findspark.add_jars(f\"{m2_local_repo}/com/nvidia/{artifactId}/{rapids_version}/{dist_jar}\")\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242319ae",
   "metadata": {},
   "source": [
    "### Configure Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15dd6b39-9f91-4855-ab13-25989937df4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for transport dt_socket at address: 5005\n",
      "23/07/31 04:54:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/31 04:54:48 WARN RapidsPluginUtils: RAPIDS Accelerator 23.10.0-SNAPSHOT using cudf 23.10.0-SNAPSHOT.\n",
      "23/07/31 04:54:48 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n",
      "23/07/31 04:54:48 WARN RapidsPluginUtils: spark.rapids.sql.explain is set to `ALL`. Set it to 'NONE' to suppress the diagnostics logging about the query placement on the GPU.\n"
     ]
    }
   ],
   "source": [
    "cores_per_exec = 1\n",
    "jdwp = '-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005'\n",
    "spark_master = f\"local[{cores_per_exec}]\"\n",
    "spark_builder = pyspark.sql.SparkSession.builder\n",
    "spark_builder.config('spark.app.name', 'RAPIDS PySpark Notebook')\n",
    "spark_builder.config('spark.driver.extraJavaOptions', f\"-Dai.rapids.cudf.preserve-dependencies=true {jdwp}\")\n",
    "spark_builder.config('spark.master', spark_master)\n",
    "spark_builder.config('spark.plugins', 'com.nvidia.spark.SQLPlugin')\n",
    "spark_builder.config('spark.rapids.sql.explain', 'ALL')\n",
    "\n",
    "spark = spark_builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31634b93",
   "metadata": {},
   "source": [
    "# Test Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2438c36-df9b-4177-85da-b114a0d16f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: string (nullable = true)\n",
      " |-- b: integer (nullable = true)\n",
      "\n",
      "23/07/31 04:54:59 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  *Exec <ProjectExec> will run on GPU\n",
      "    *Expression <Alias> cast(b#1 as string) AS b#7 will run on GPU\n",
      "      *Expression <Cast> cast(b#1 as string) will run on GPU\n",
      "    ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "      @Expression <AttributeReference> a#0 could run on GPU\n",
      "      @Expression <AttributeReference> b#1 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|a         |b  |\n",
      "+----------+---+\n",
      "|gera      |2  |\n",
      "|231       |10 |\n",
      "|0xDEADBEEF|16 |\n",
      "+----------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('gera', 2), ('231', 10), ('0xDEADBEEF', 16),], 'a string, b integer')\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"TBL\")\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd76204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Chip(Enum):\n",
    "    CPU = 1\n",
    "    GPU = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c142cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_case(chip):\n",
    "    print(f\"##### TEST {chip=}\\n\")\n",
    "    spark.conf.set('spark.rapids.sql.enabled', chip == Chip.GPU)\n",
    "    print(\"### original\")\n",
    "    print(df.collect())\n",
    "    print(\"### non-literal from_base\")\n",
    "    print(spark.sql(\"SELECT CONV(a, b, 16) FROM TBL\").collect())\n",
    "    print(\"### 10 -> 16\")\n",
    "    print(df.select(pyspark.sql.functions.conv('a', 10, 16)).collect())\n",
    "    print(\"### 16 -> 10\")\n",
    "    print(df.select(pyspark.sql.functions.conv('a', 16, 10)).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c616d4",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65e99255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### TEST chip=<Chip.GPU: 2>\n",
      "\n",
      "### original\n",
      "23/07/31 04:55:01 WARN GpuOverrides: \n",
      "! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "  @Expression <AttributeReference> a#0 could run on GPU\n",
      "  @Expression <AttributeReference> b#1 could run on GPU\n",
      "\n",
      "[Row(a='gera', b=2), Row(a='231', b=10), Row(a='0xDEADBEEF', b=16)]\n",
      "### non-literal from_base\n",
      "23/07/31 04:55:01 WARN GpuOverrides: \n",
      "!Exec <ProjectExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <Alias> conv(a#0, b#1, 16) AS conv(a, b, 16)#13 could run on GPU\n",
      "    !Expression <Conv> conv(a#0, b#1, 16) cannot run on GPU because only liter from_base and to_base are supported\n",
      "      @Expression <AttributeReference> a#0 could run on GPU\n",
      "      @Expression <AttributeReference> b#1 could run on GPU\n",
      "      @Expression <Literal> 16 could run on GPU\n",
      "  ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "    @Expression <AttributeReference> a#0 could run on GPU\n",
      "    @Expression <AttributeReference> b#1 could run on GPU\n",
      "\n",
      "[Row(conv(a, b, 16)='0'), Row(conv(a, b, 16)='E7'), Row(conv(a, b, 16)='0')]\n",
      "### 10 -> 16\n",
      "23/07/31 04:55:01 WARN GpuOverrides: \n",
      "*Exec <ProjectExec> will run on GPU\n",
      "  *Expression <Alias> conv(a#0, 10, 16) AS conv(a, 10, 16)#15 will run on GPU\n",
      "    *Expression <Conv> conv(a#0, 10, 16) will run on GPU\n",
      "  ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "    @Expression <AttributeReference> a#0 could run on GPU\n",
      "    @Expression <AttributeReference> b#1 could run on GPU\n",
      "\n",
      "23/07/31 04:55:02 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)\n",
      "java.lang.UnsupportedOperationException\n",
      "\tat org.apache.spark.sql.rapids.GpuConv.doColumnar(stringFunctions.scala:2290)\n",
      "\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$8(GpuExpressions.scala:413)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n",
      "\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$7(GpuExpressions.scala:398)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n",
      "\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$6(GpuExpressions.scala:397)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n",
      "\tat com.nvidia.spark.rapids.GpuTernaryExpression.columnarEval(GpuExpressions.scala:396)\n",
      "\tat com.nvidia.spark.rapids.GpuTernaryExpression.columnarEval$(GpuExpressions.scala:394)\n",
      "\tat org.apache.spark.sql.rapids.GpuConv.columnarEval(stringFunctions.scala:2247)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$ReallyAGpuExpression.columnarEval(implicits.scala:34)\n",
      "\tat com.nvidia.spark.rapids.GpuAlias.columnarEval(namedExpressions.scala:110)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$ReallyAGpuExpression.columnarEval(implicits.scala:34)\n",
      "\tat com.nvidia.spark.rapids.GpuProjectExec$.$anonfun$project$1(basicPhysicalOperators.scala:108)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1(implicits.scala:220)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1$adapted(implicits.scala:217)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.safeMap(implicits.scala:217)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$AutoCloseableProducingSeq.safeMap(implicits.scala:252)\n",
      "\tat com.nvidia.spark.rapids.GpuProjectExec$.project(basicPhysicalOperators.scala:108)\n",
      "\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$project$2(basicPhysicalOperators.scala:595)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.GpuTieredProject.recurse$2(basicPhysicalOperators.scala:594)\n",
      "\tat com.nvidia.spark.rapids.GpuTieredProject.project(basicPhysicalOperators.scala:607)\n",
      "\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$projectWithRetrySingleBatchInternal$2(basicPhysicalOperators.scala:538)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$projectWithRetrySingleBatchInternal$1(basicPhysicalOperators.scala:537)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:431)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:542)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:468)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.drainSingleWithVerification(RmmRapidsRetryIterator.scala:275)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.withRetryNoSplit(RmmRapidsRetryIterator.scala:128)\n",
      "\tat com.nvidia.spark.rapids.GpuTieredProject.projectWithRetrySingleBatchInternal(basicPhysicalOperators.scala:536)\n",
      "\tat com.nvidia.spark.rapids.GpuTieredProject.projectAndCloseWithRetrySingleBatch(basicPhysicalOperators.scala:577)\n",
      "\tat com.nvidia.spark.rapids.GpuProjectExec.$anonfun$internalDoExecuteColumnar$2(basicPhysicalOperators.scala:377)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.GpuProjectExec.$anonfun$internalDoExecuteColumnar$1(basicPhysicalOperators.scala:373)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat com.nvidia.spark.rapids.ColumnarToRowIterator.$anonfun$fetchNextBatch$2(GpuColumnarToRowExec.scala:258)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.ColumnarToRowIterator.fetchNextBatch(GpuColumnarToRowExec.scala:255)\n",
      "\tat com.nvidia.spark.rapids.ColumnarToRowIterator.loadNextBatch(GpuColumnarToRowExec.scala:228)\n",
      "\tat com.nvidia.spark.rapids.ColumnarToRowIterator.hasNext(GpuColumnarToRowExec.scala:272)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/07/31 04:55:02 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) (localhost executor driver): java.lang.UnsupportedOperationException\n",
      "\tat org.apache.spark.sql.rapids.GpuConv.doColumnar(stringFunctions.scala:2290)\n",
      "\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$8(GpuExpressions.scala:413)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n",
      "\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$7(GpuExpressions.scala:398)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n",
      "\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$6(GpuExpressions.scala:397)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n",
      "\tat com.nvidia.spark.rapids.GpuTernaryExpression.columnarEval(GpuExpressions.scala:396)\n",
      "\tat com.nvidia.spark.rapids.GpuTernaryExpression.columnarEval$(GpuExpressions.scala:394)\n",
      "\tat org.apache.spark.sql.rapids.GpuConv.columnarEval(stringFunctions.scala:2247)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$ReallyAGpuExpression.columnarEval(implicits.scala:34)\n",
      "\tat com.nvidia.spark.rapids.GpuAlias.columnarEval(namedExpressions.scala:110)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$ReallyAGpuExpression.columnarEval(implicits.scala:34)\n",
      "\tat com.nvidia.spark.rapids.GpuProjectExec$.$anonfun$project$1(basicPhysicalOperators.scala:108)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1(implicits.scala:220)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1$adapted(implicits.scala:217)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.safeMap(implicits.scala:217)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$AutoCloseableProducingSeq.safeMap(implicits.scala:252)\n",
      "\tat com.nvidia.spark.rapids.GpuProjectExec$.project(basicPhysicalOperators.scala:108)\n",
      "\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$project$2(basicPhysicalOperators.scala:595)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.GpuTieredProject.recurse$2(basicPhysicalOperators.scala:594)\n",
      "\tat com.nvidia.spark.rapids.GpuTieredProject.project(basicPhysicalOperators.scala:607)\n",
      "\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$projectWithRetrySingleBatchInternal$2(basicPhysicalOperators.scala:538)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$projectWithRetrySingleBatchInternal$1(basicPhysicalOperators.scala:537)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:431)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:542)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:468)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.drainSingleWithVerification(RmmRapidsRetryIterator.scala:275)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.withRetryNoSplit(RmmRapidsRetryIterator.scala:128)\n",
      "\tat com.nvidia.spark.rapids.GpuTieredProject.projectWithRetrySingleBatchInternal(basicPhysicalOperators.scala:536)\n",
      "\tat com.nvidia.spark.rapids.GpuTieredProject.projectAndCloseWithRetrySingleBatch(basicPhysicalOperators.scala:577)\n",
      "\tat com.nvidia.spark.rapids.GpuProjectExec.$anonfun$internalDoExecuteColumnar$2(basicPhysicalOperators.scala:377)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.GpuProjectExec.$anonfun$internalDoExecuteColumnar$1(basicPhysicalOperators.scala:373)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat com.nvidia.spark.rapids.ColumnarToRowIterator.$anonfun$fetchNextBatch$2(GpuColumnarToRowExec.scala:258)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.ColumnarToRowIterator.fetchNextBatch(GpuColumnarToRowExec.scala:255)\n",
      "\tat com.nvidia.spark.rapids.ColumnarToRowIterator.loadNextBatch(GpuColumnarToRowExec.scala:228)\n",
      "\tat com.nvidia.spark.rapids.ColumnarToRowIterator.hasNext(GpuColumnarToRowExec.scala:272)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/07/31 04:55:02 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o58.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (localhost executor driver): java.lang.UnsupportedOperationException\n\tat org.apache.spark.sql.rapids.GpuConv.doColumnar(stringFunctions.scala:2290)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$8(GpuExpressions.scala:413)\n\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$7(GpuExpressions.scala:398)\n\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$6(GpuExpressions.scala:397)\n\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.columnarEval(GpuExpressions.scala:396)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.columnarEval$(GpuExpressions.scala:394)\n\tat org.apache.spark.sql.rapids.GpuConv.columnarEval(stringFunctions.scala:2247)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$ReallyAGpuExpression.columnarEval(implicits.scala:34)\n\tat com.nvidia.spark.rapids.GpuAlias.columnarEval(namedExpressions.scala:110)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$ReallyAGpuExpression.columnarEval(implicits.scala:34)\n\tat com.nvidia.spark.rapids.GpuProjectExec$.$anonfun$project$1(basicPhysicalOperators.scala:108)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1(implicits.scala:220)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1$adapted(implicits.scala:217)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.safeMap(implicits.scala:217)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$AutoCloseableProducingSeq.safeMap(implicits.scala:252)\n\tat com.nvidia.spark.rapids.GpuProjectExec$.project(basicPhysicalOperators.scala:108)\n\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$project$2(basicPhysicalOperators.scala:595)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuTieredProject.recurse$2(basicPhysicalOperators.scala:594)\n\tat com.nvidia.spark.rapids.GpuTieredProject.project(basicPhysicalOperators.scala:607)\n\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$projectWithRetrySingleBatchInternal$2(basicPhysicalOperators.scala:538)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$projectWithRetrySingleBatchInternal$1(basicPhysicalOperators.scala:537)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:431)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:542)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:468)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.drainSingleWithVerification(RmmRapidsRetryIterator.scala:275)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.withRetryNoSplit(RmmRapidsRetryIterator.scala:128)\n\tat com.nvidia.spark.rapids.GpuTieredProject.projectWithRetrySingleBatchInternal(basicPhysicalOperators.scala:536)\n\tat com.nvidia.spark.rapids.GpuTieredProject.projectAndCloseWithRetrySingleBatch(basicPhysicalOperators.scala:577)\n\tat com.nvidia.spark.rapids.GpuProjectExec.$anonfun$internalDoExecuteColumnar$2(basicPhysicalOperators.scala:377)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuProjectExec.$anonfun$internalDoExecuteColumnar$1(basicPhysicalOperators.scala:373)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.$anonfun$fetchNextBatch$2(GpuColumnarToRowExec.scala:258)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.fetchNextBatch(GpuColumnarToRowExec.scala:255)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.loadNextBatch(GpuColumnarToRowExec.scala:228)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.hasNext(GpuColumnarToRowExec.scala:272)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.UnsupportedOperationException\n\tat org.apache.spark.sql.rapids.GpuConv.doColumnar(stringFunctions.scala:2290)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$8(GpuExpressions.scala:413)\n\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$7(GpuExpressions.scala:398)\n\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$6(GpuExpressions.scala:397)\n\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.columnarEval(GpuExpressions.scala:396)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.columnarEval$(GpuExpressions.scala:394)\n\tat org.apache.spark.sql.rapids.GpuConv.columnarEval(stringFunctions.scala:2247)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$ReallyAGpuExpression.columnarEval(implicits.scala:34)\n\tat com.nvidia.spark.rapids.GpuAlias.columnarEval(namedExpressions.scala:110)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$ReallyAGpuExpression.columnarEval(implicits.scala:34)\n\tat com.nvidia.spark.rapids.GpuProjectExec$.$anonfun$project$1(basicPhysicalOperators.scala:108)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1(implicits.scala:220)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1$adapted(implicits.scala:217)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.safeMap(implicits.scala:217)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$AutoCloseableProducingSeq.safeMap(implicits.scala:252)\n\tat com.nvidia.spark.rapids.GpuProjectExec$.project(basicPhysicalOperators.scala:108)\n\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$project$2(basicPhysicalOperators.scala:595)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuTieredProject.recurse$2(basicPhysicalOperators.scala:594)\n\tat com.nvidia.spark.rapids.GpuTieredProject.project(basicPhysicalOperators.scala:607)\n\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$projectWithRetrySingleBatchInternal$2(basicPhysicalOperators.scala:538)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$projectWithRetrySingleBatchInternal$1(basicPhysicalOperators.scala:537)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:431)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:542)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:468)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.drainSingleWithVerification(RmmRapidsRetryIterator.scala:275)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.withRetryNoSplit(RmmRapidsRetryIterator.scala:128)\n\tat com.nvidia.spark.rapids.GpuTieredProject.projectWithRetrySingleBatchInternal(basicPhysicalOperators.scala:536)\n\tat com.nvidia.spark.rapids.GpuTieredProject.projectAndCloseWithRetrySingleBatch(basicPhysicalOperators.scala:577)\n\tat com.nvidia.spark.rapids.GpuProjectExec.$anonfun$internalDoExecuteColumnar$2(basicPhysicalOperators.scala:377)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuProjectExec.$anonfun$internalDoExecuteColumnar$1(basicPhysicalOperators.scala:373)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.$anonfun$fetchNextBatch$2(GpuColumnarToRowExec.scala:258)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.fetchNextBatch(GpuColumnarToRowExec.scala:255)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.loadNextBatch(GpuColumnarToRowExec.scala:228)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.hasNext(GpuColumnarToRowExec.scala:272)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_case(chip \u001b[39m=\u001b[39;49m Chip\u001b[39m.\u001b[39;49mGPU)\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36mtest_case\u001b[0;34m(chip)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(spark\u001b[39m.\u001b[39msql(\u001b[39m\"\u001b[39m\u001b[39mSELECT CONV(a, b, 16) FROM TBL\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcollect())\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m### 10 -> 16\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39;49mselect(pyspark\u001b[39m.\u001b[39;49msql\u001b[39m.\u001b[39;49mfunctions\u001b[39m.\u001b[39;49mconv(\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m10\u001b[39;49m, \u001b[39m16\u001b[39;49m))\u001b[39m.\u001b[39;49mcollect())\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m### 16 -> 10\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39mselect(pyspark\u001b[39m.\u001b[39msql\u001b[39m.\u001b[39mfunctions\u001b[39m.\u001b[39mconv(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m10\u001b[39m))\u001b[39m.\u001b[39mcollect())\n",
      "File \u001b[0;32m~/dist/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[39m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \n\u001b[1;32m    809\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc):\n\u001b[0;32m--> 817\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcollectToPython()\n\u001b[1;32m    818\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/dist/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/dist/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/dist/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o58.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (localhost executor driver): java.lang.UnsupportedOperationException\n\tat org.apache.spark.sql.rapids.GpuConv.doColumnar(stringFunctions.scala:2290)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$8(GpuExpressions.scala:413)\n\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$7(GpuExpressions.scala:398)\n\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$6(GpuExpressions.scala:397)\n\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.columnarEval(GpuExpressions.scala:396)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.columnarEval$(GpuExpressions.scala:394)\n\tat org.apache.spark.sql.rapids.GpuConv.columnarEval(stringFunctions.scala:2247)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$ReallyAGpuExpression.columnarEval(implicits.scala:34)\n\tat com.nvidia.spark.rapids.GpuAlias.columnarEval(namedExpressions.scala:110)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$ReallyAGpuExpression.columnarEval(implicits.scala:34)\n\tat com.nvidia.spark.rapids.GpuProjectExec$.$anonfun$project$1(basicPhysicalOperators.scala:108)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1(implicits.scala:220)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1$adapted(implicits.scala:217)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.safeMap(implicits.scala:217)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$AutoCloseableProducingSeq.safeMap(implicits.scala:252)\n\tat com.nvidia.spark.rapids.GpuProjectExec$.project(basicPhysicalOperators.scala:108)\n\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$project$2(basicPhysicalOperators.scala:595)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuTieredProject.recurse$2(basicPhysicalOperators.scala:594)\n\tat com.nvidia.spark.rapids.GpuTieredProject.project(basicPhysicalOperators.scala:607)\n\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$projectWithRetrySingleBatchInternal$2(basicPhysicalOperators.scala:538)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$projectWithRetrySingleBatchInternal$1(basicPhysicalOperators.scala:537)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:431)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:542)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:468)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.drainSingleWithVerification(RmmRapidsRetryIterator.scala:275)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.withRetryNoSplit(RmmRapidsRetryIterator.scala:128)\n\tat com.nvidia.spark.rapids.GpuTieredProject.projectWithRetrySingleBatchInternal(basicPhysicalOperators.scala:536)\n\tat com.nvidia.spark.rapids.GpuTieredProject.projectAndCloseWithRetrySingleBatch(basicPhysicalOperators.scala:577)\n\tat com.nvidia.spark.rapids.GpuProjectExec.$anonfun$internalDoExecuteColumnar$2(basicPhysicalOperators.scala:377)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuProjectExec.$anonfun$internalDoExecuteColumnar$1(basicPhysicalOperators.scala:373)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.$anonfun$fetchNextBatch$2(GpuColumnarToRowExec.scala:258)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.fetchNextBatch(GpuColumnarToRowExec.scala:255)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.loadNextBatch(GpuColumnarToRowExec.scala:228)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.hasNext(GpuColumnarToRowExec.scala:272)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.UnsupportedOperationException\n\tat org.apache.spark.sql.rapids.GpuConv.doColumnar(stringFunctions.scala:2290)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$8(GpuExpressions.scala:413)\n\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$7(GpuExpressions.scala:398)\n\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.$anonfun$columnarEval$6(GpuExpressions.scala:397)\n\tat com.nvidia.spark.rapids.Arm$.withResourceIfAllowed(Arm.scala:74)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.columnarEval(GpuExpressions.scala:396)\n\tat com.nvidia.spark.rapids.GpuTernaryExpression.columnarEval$(GpuExpressions.scala:394)\n\tat org.apache.spark.sql.rapids.GpuConv.columnarEval(stringFunctions.scala:2247)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$ReallyAGpuExpression.columnarEval(implicits.scala:34)\n\tat com.nvidia.spark.rapids.GpuAlias.columnarEval(namedExpressions.scala:110)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$ReallyAGpuExpression.columnarEval(implicits.scala:34)\n\tat com.nvidia.spark.rapids.GpuProjectExec$.$anonfun$project$1(basicPhysicalOperators.scala:108)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1(implicits.scala:220)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1$adapted(implicits.scala:217)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.safeMap(implicits.scala:217)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$AutoCloseableProducingSeq.safeMap(implicits.scala:252)\n\tat com.nvidia.spark.rapids.GpuProjectExec$.project(basicPhysicalOperators.scala:108)\n\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$project$2(basicPhysicalOperators.scala:595)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuTieredProject.recurse$2(basicPhysicalOperators.scala:594)\n\tat com.nvidia.spark.rapids.GpuTieredProject.project(basicPhysicalOperators.scala:607)\n\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$projectWithRetrySingleBatchInternal$2(basicPhysicalOperators.scala:538)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuTieredProject.$anonfun$projectWithRetrySingleBatchInternal$1(basicPhysicalOperators.scala:537)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:431)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:542)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:468)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.drainSingleWithVerification(RmmRapidsRetryIterator.scala:275)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.withRetryNoSplit(RmmRapidsRetryIterator.scala:128)\n\tat com.nvidia.spark.rapids.GpuTieredProject.projectWithRetrySingleBatchInternal(basicPhysicalOperators.scala:536)\n\tat com.nvidia.spark.rapids.GpuTieredProject.projectAndCloseWithRetrySingleBatch(basicPhysicalOperators.scala:577)\n\tat com.nvidia.spark.rapids.GpuProjectExec.$anonfun$internalDoExecuteColumnar$2(basicPhysicalOperators.scala:377)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuProjectExec.$anonfun$internalDoExecuteColumnar$1(basicPhysicalOperators.scala:373)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.$anonfun$fetchNextBatch$2(GpuColumnarToRowExec.scala:258)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.fetchNextBatch(GpuColumnarToRowExec.scala:255)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.loadNextBatch(GpuColumnarToRowExec.scala:228)\n\tat com.nvidia.spark.rapids.ColumnarToRowIterator.hasNext(GpuColumnarToRowExec.scala:272)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "test_case(chip = Chip.GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e085af",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb78f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case(chip = Chip.CPU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "24e1e440b58174d23459f334e6373b3f84a303d378405919ee47af328df355be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
