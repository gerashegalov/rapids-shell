{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4146aabb-1c55-415e-b60c-23a1dcc1184a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Start jupyter-lab\n",
    "\n",
    "```bash\n",
    "jupyter-lab --notebook-dir=$HOME/gits/gerashegalov/rapids-shell/src/jupyter\n",
    "```\n",
    "or simply open in VS Code with Jupyter extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9c9ac",
   "metadata": {},
   "source": [
    "# Repro for [NVIDIA/spark-rapids#8625](https://github.com/NVIDIA/spark-rapids/issues/8625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4010da20-e354-4b10-b3ce-000344c59daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import fastparquet\n",
    "import findspark\n",
    "import glob\n",
    "import os\n",
    "import pyarrow\n",
    "import pyarrow.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72923f92",
   "metadata": {},
   "source": [
    "### Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152499e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rapids_version = '23.08.0-SNAPSHOT'\n",
    "spark_version = '3.4.1'\n",
    "cuda_version = 'cuda11'\n",
    "scala_version = '2.12'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84d10a",
   "metadata": {},
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1b741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
    "os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'\n",
    "os.environ['TZ'] = 'UTC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab907e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.environ['HOME']\n",
    "work_dir = f\"{home_dir}/jupyter_run_dir\"\n",
    "rapids_home = f\"{home_dir}/gits/NVIDIA/spark-rapids\"\n",
    "dist_jar = f\"rapids-4-spark_{scala_version}-{rapids_version}-{cuda_version}.jar\"\n",
    "spark_home = f\"{home_dir}/dist/spark-{spark_version}-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29534b21",
   "metadata": {},
   "source": [
    "### Find Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1dfd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(spark_home = spark_home)\n",
    "findspark.add_jars(f\"{rapids_home}/dist/target/{dist_jar}\")\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242319ae",
   "metadata": {},
   "source": [
    "### Configure Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15dd6b39-9f91-4855-ab13-25989937df4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/19 21:18:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/19 21:18:59 WARN RapidsPluginUtils: RAPIDS Accelerator 23.08.0-SNAPSHOT using cudf 23.08.0-GSHEGALOV-DT.\n",
      "23/07/19 21:18:59 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n",
      "23/07/19 21:18:59 WARN RapidsPluginUtils: spark.rapids.sql.explain is set to `ALL`. Set it to 'NONE' to suppress the diagnostics logging about the query placement on the GPU.\n"
     ]
    }
   ],
   "source": [
    "cores_per_exec = 1\n",
    "spark_master = f\"local[{cores_per_exec}]\"\n",
    "spark_builder = pyspark.sql.SparkSession.builder.config(\n",
    "    map = {\n",
    "        'spark.app.name': 'RAPIDS PySpark Notebook',\n",
    "        'spark.driver.extraJavaOptions': '-Dai.rapids.cudf.preserve-dependencies=true',\n",
    "        'spark.master': spark_master,\n",
    "        'spark.plugins': 'com.nvidia.spark.SQLPlugin',\n",
    "        'spark.rapids.sql.enabled': True,\n",
    "        'spark.rapids.sql.explain': 'ALL', \n",
    "    }   \n",
    ")\n",
    "spark = spark_builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31634b93",
   "metadata": {},
   "source": [
    "# Test Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd3a6125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2023, 7, 14, 7, 38, 45, 418688),\n",
       " datetime.datetime(3023, 7, 14, 7, 38, 45, 418688))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_tup = (\n",
    "    datetime.datetime(2023, 7, 14, 7, 38, 45, 418688),\n",
    "    datetime.datetime(3023, 7, 14, 7, 38, 45, 418688),\n",
    ")\n",
    "ts_tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2438c36-df9b-4177-85da-b114a0d16f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- big_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/19 21:19:10 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  *Exec <ProjectExec> will run on GPU\n",
      "    *Expression <Alias> cast(ts#0 as string) AS ts#6 will run on GPU\n",
      "      *Expression <Cast> cast(ts#0 as string) will run on GPU\n",
      "    *Expression <Alias> cast(big_ts#1 as string) AS big_ts#7 will run on GPU\n",
      "      *Expression <Cast> cast(big_ts#1 as string) will run on GPU\n",
      "    ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "      @Expression <AttributeReference> ts#0 could run on GPU\n",
      "      @Expression <AttributeReference> big_ts#1 could run on GPU\n",
      "\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------------------------+\n",
      "|ts                        |big_ts                    |\n",
      "+--------------------------+--------------------------+\n",
      "|2023-07-14 07:38:45.418688|3023-07-14 07:38:45.418688|\n",
      "+--------------------------+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([ts_tup,], 'ts timestamp, big_ts timestamp')\n",
    "df1.createOrReplaceTempView('df1')\n",
    "df1.printSchema()\n",
    "df1.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b65b29aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INT96'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.parquet.outputTimestampType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee003ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_path = '/tmp/int96_out_cpu.parquet'\n",
    "gpu_path = '/tmp/int96_out_gpu.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c616d4",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65e99255",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.rapids.sql.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "260b6007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/19 21:19:11 WARN GpuOverrides: \n",
      "*Exec <DataWritingCommandExec> will run on GPU\n",
      "  *Output <InsertIntoHadoopFsRelationCommand> will run on GPU\n",
      "  *Exec <WriteFilesExec> will run on GPU\n",
      "    *Exec <HashAggregateExec> will run on GPU\n",
      "      *Expression <AggregateExpression> max(ts#0) will run on GPU\n",
      "        *Expression <Max> max(ts#0) will run on GPU\n",
      "      *Expression <AggregateExpression> max(big_ts#1) will run on GPU\n",
      "        *Expression <Max> max(big_ts#1) will run on GPU\n",
      "      *Expression <Alias> max(ts#0)#15 AS max_ts#13 will run on GPU\n",
      "      *Expression <Alias> max(big_ts#1)#16 AS max_big_ts#14 will run on GPU\n",
      "      *Exec <ShuffleExchangeExec> will run on GPU\n",
      "        *Partitioning <SinglePartition$> will run on GPU\n",
      "        *Exec <HashAggregateExec> will run on GPU\n",
      "          *Expression <AggregateExpression> partial_max(ts#0) will run on GPU\n",
      "            *Expression <Max> max(ts#0) will run on GPU\n",
      "          *Expression <AggregateExpression> partial_max(big_ts#1) will run on GPU\n",
      "            *Expression <Max> max(big_ts#1) will run on GPU\n",
      "          ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "            @Expression <AttributeReference> ts#0 could run on GPU\n",
      "            @Expression <AttributeReference> big_ts#1 could run on GPU\n",
      "\n",
      "23/07/19 21:19:11 WARN GpuOverrides: \n",
      "*Exec <DataWritingCommandExec> will run on GPU\n",
      "  *Output <InsertIntoHadoopFsRelationCommand> will run on GPU\n",
      "  *Exec <WriteFilesExec> will run on GPU\n",
      "    *Exec <HashAggregateExec> will run on GPU\n",
      "      *Expression <AggregateExpression> max(ts#0) will run on GPU\n",
      "        *Expression <Max> max(ts#0) will run on GPU\n",
      "      *Expression <AggregateExpression> max(big_ts#1) will run on GPU\n",
      "        *Expression <Max> max(big_ts#1) will run on GPU\n",
      "      *Expression <Alias> max(ts#0)#15 AS max_ts#13 will run on GPU\n",
      "      *Expression <Alias> max(big_ts#1)#16 AS max_big_ts#14 will run on GPU\n",
      "      *Exec <ShuffleExchangeExec> will run on GPU\n",
      "        *Partitioning <SinglePartition$> will run on GPU\n",
      "        *Exec <HashAggregateExec> will run on GPU\n",
      "          *Expression <AggregateExpression> partial_max(ts#0) will run on GPU\n",
      "            *Expression <Max> max(ts#0) will run on GPU\n",
      "          *Expression <AggregateExpression> partial_max(big_ts#1) will run on GPU\n",
      "            *Expression <Max> max(big_ts#1) will run on GPU\n",
      "          ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "            @Expression <AttributeReference> ts#0 could run on GPU\n",
      "            @Expression <AttributeReference> big_ts#1 could run on GPU\n",
      "\n",
      "23/07/19 21:19:11 WARN GpuOverrides: \n",
      "*Exec <DataWritingCommandExec> will run on GPU\n",
      "  *Output <InsertIntoHadoopFsRelationCommand> will run on GPU\n",
      "  *Exec <WriteFilesExec> will run on GPU\n",
      "    *Exec <HashAggregateExec> will run on GPU\n",
      "      *Expression <AggregateExpression> max(ts#0) will run on GPU\n",
      "        *Expression <Max> max(ts#0) will run on GPU\n",
      "      *Expression <AggregateExpression> max(big_ts#1) will run on GPU\n",
      "        *Expression <Max> max(big_ts#1) will run on GPU\n",
      "      *Expression <Alias> max(ts#0)#15 AS max_ts#13 will run on GPU\n",
      "      *Expression <Alias> max(big_ts#1)#16 AS max_big_ts#14 will run on GPU\n",
      "      *Exec <ShuffleExchangeExec> will run on GPU\n",
      "        *Partitioning <SinglePartition$> will run on GPU\n",
      "        *Exec <HashAggregateExec> will run on GPU\n",
      "          *Expression <AggregateExpression> partial_max(ts#0) will run on GPU\n",
      "            *Expression <Max> max(ts#0) will run on GPU\n",
      "          *Expression <AggregateExpression> partial_max(big_ts#1) will run on GPU\n",
      "            *Expression <Max> max(big_ts#1) will run on GPU\n",
      "          ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "            @Expression <AttributeReference> ts#0 could run on GPU\n",
      "            @Expression <AttributeReference> big_ts#1 could run on GPU\n",
      "\n",
      "23/07/19 21:19:11 WARN GpuOverrides: \n",
      "*Exec <ShuffleExchangeExec> will run on GPU\n",
      "  *Partitioning <SinglePartition$> will run on GPU\n",
      "  *Exec <HashAggregateExec> will run on GPU\n",
      "    *Expression <AggregateExpression> partial_max(ts#0) will run on GPU\n",
      "      *Expression <Max> max(ts#0) will run on GPU\n",
      "    *Expression <AggregateExpression> partial_max(big_ts#1) will run on GPU\n",
      "      *Expression <Max> max(big_ts#1) will run on GPU\n",
      "    ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "      @Expression <AttributeReference> ts#0 could run on GPU\n",
      "      @Expression <AttributeReference> big_ts#1 could run on GPU\n",
      "\n",
      "23/07/19 21:19:12 WARN GpuOverrides: \n",
      "*Exec <DataWritingCommandExec> will run on GPU\n",
      "  *Output <InsertIntoHadoopFsRelationCommand> will run on GPU\n",
      "  *Exec <WriteFilesExec> will run on GPU\n",
      "    *Exec <HashAggregateExec> will run on GPU\n",
      "      *Expression <AggregateExpression> max(ts#0) will run on GPU\n",
      "        *Expression <Max> max(ts#0) will run on GPU\n",
      "      *Expression <AggregateExpression> max(big_ts#1) will run on GPU\n",
      "        *Expression <Max> max(big_ts#1) will run on GPU\n",
      "      *Expression <Alias> max(ts#0)#15 AS max_ts#13 will run on GPU\n",
      "      *Expression <Alias> max(big_ts#1)#16 AS max_big_ts#14 will run on GPU\n",
      "\n",
      "23/07/19 21:19:12 WARN GpuOverrides: \n",
      "*Exec <DataWritingCommandExec> will run on GPU\n",
      "  *Output <InsertIntoHadoopFsRelationCommand> will run on GPU\n",
      "  *Exec <WriteFilesExec> will run on GPU\n",
      "    *Exec <HashAggregateExec> will run on GPU\n",
      "      *Expression <AggregateExpression> max(ts#0) will run on GPU\n",
      "        *Expression <Max> max(ts#0) will run on GPU\n",
      "      *Expression <AggregateExpression> max(big_ts#1) will run on GPU\n",
      "        *Expression <Max> max(big_ts#1) will run on GPU\n",
      "      *Expression <Alias> max(ts#0)#15 AS max_ts#13 will run on GPU\n",
      "      *Expression <Alias> max(big_ts#1)#16 AS max_big_ts#14 will run on GPU\n",
      "\n",
      "GERA_DEBUG (L2309) writer::writer int96_ts=0\n",
      "GERA_DEBUG (L1676) Java_ai_rapids_cudf_Table_writeParquetChunk\n",
      "GERA_DEBUG io::detail:parquet::write\n",
      "GERA_DEBUG (L1454): 15\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT MAX(ts) as max_ts, MAX(big_ts) as max_big_ts FROM df1\").write.mode('overwrite').parquet(gpu_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "079f0c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- max_ts: timestamp (nullable = true)\n",
      " |-- max_big_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/19 21:19:13 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  *Exec <ProjectExec> will run on GPU\n",
      "    *Expression <Alias> cast(max_ts#65 as string) AS max_ts#71 will run on GPU\n",
      "      *Expression <Cast> cast(max_ts#65 as string) will run on GPU\n",
      "    *Expression <Alias> cast(max_big_ts#66 as string) AS max_big_ts#72 will run on GPU\n",
      "      *Expression <Cast> cast(max_big_ts#66 as string) will run on GPU\n",
      "    *Exec <FileSourceScanExec> will run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------------------------+\n",
      "|max_ts                    |max_big_ts                |\n",
      "+--------------------------+--------------------------+\n",
      "|2023-07-14 07:38:45.418688|1854-06-04 08:29:37.999584|\n",
      "+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf = spark.read.parquet(gpu_path)\n",
    "gdf.printSchema()\n",
    "gdf.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddaaba6",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad4c5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.rapids.sql.enabled', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c61bf194",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT MAX(ts) as max_ts, MAX(big_ts) as max_big_ts FROM df1\").write.mode('overwrite').parquet(cpu_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6a81212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- max_ts: timestamp (nullable = true)\n",
      " |-- max_big_ts: timestamp (nullable = true)\n",
      "\n",
      "+--------------------------+--------------------------+\n",
      "|max_ts                    |max_big_ts                |\n",
      "+--------------------------+--------------------------+\n",
      "|2023-07-14 07:38:45.418688|3023-07-14 07:38:45.418688|\n",
      "+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf = spark.read.parquet(cpu_path)\n",
    "gdf.printSchema()\n",
    "gdf.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc4c9f9",
   "metadata": {},
   "source": [
    "## Other Readers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db03501b",
   "metadata": {},
   "source": [
    "### Read Spark GPU output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7b77dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_parquet_file, = glob.glob(f\"{gpu_path}/*.parquet\")\n",
    "cpu_parquet_file, = glob.glob(f\"{cpu_path}/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d62f3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_by': '',\n",
       " 'num_columns': 2,\n",
       " 'num_rows': 1,\n",
       " 'num_row_groups': 1,\n",
       " 'row_groups': [{'num_columns': 2,\n",
       "   'num_rows': 1,\n",
       "   'total_byte_size': 70,\n",
       "   'columns': [{'file_offset': 0,\n",
       "     'file_path': '',\n",
       "     'physical_type': 'INT96',\n",
       "     'num_values': 1,\n",
       "     'path_in_schema': 'max_ts',\n",
       "     'is_stats_set': False,\n",
       "     'statistics': None,\n",
       "     'compression': 'UNCOMPRESSED',\n",
       "     'encodings': ('PLAIN', 'RLE'),\n",
       "     'has_dictionary_page': False,\n",
       "     'dictionary_page_offset': None,\n",
       "     'data_page_offset': 4,\n",
       "     'total_compressed_size': 35,\n",
       "     'total_uncompressed_size': 35},\n",
       "    {'file_offset': 0,\n",
       "     'file_path': '',\n",
       "     'physical_type': 'INT96',\n",
       "     'num_values': 1,\n",
       "     'path_in_schema': 'max_big_ts',\n",
       "     'is_stats_set': False,\n",
       "     'statistics': None,\n",
       "     'compression': 'UNCOMPRESSED',\n",
       "     'encodings': ('PLAIN', 'RLE'),\n",
       "     'has_dictionary_page': False,\n",
       "     'dictionary_page_offset': None,\n",
       "     'data_page_offset': 39,\n",
       "     'total_compressed_size': 35,\n",
       "     'total_uncompressed_size': 35}]}],\n",
       " 'format_version': '1.0',\n",
       " 'serialized_size': 427}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmd = pyarrow.parquet.read_metadata(gpu_parquet_file)\n",
    "pmd.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2d29efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.version\n",
      "3.4.1\n",
      "org.apache.spark.sql.parquet.row.metadata\n",
      "{\"type\":\"struct\",\"fields\":[{\"name\":\"max_ts\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}},{\"name\":\"max_big_ts\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}]}\n"
     ]
    }
   ],
   "source": [
    "for k, v in pmd.metadata.items():\n",
    "    print(k.decode('ascii'))\n",
    "    print(v.decode('ascii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a85d4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.ParquetSchema object at 0x7f347552d780>\n",
       "required group field_id=-1 schema {\n",
       "  optional int96 field_id=-1 max_ts;\n",
       "  optional int96 field_id=-1 max_big_ts;\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmd.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e8c96a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "max_ts: timestamp[ns]\n",
       "max_big_ts: timestamp[ns]\n",
       "----\n",
       "max_ts: [[2023-07-14 07:38:45.418688000]]\n",
       "max_big_ts: [[1854-06-04 08:29:37.999584768]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyarrow.parquet.read_table(gpu_parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dab878",
   "metadata": {},
   "source": [
    "### Read Spark CPU output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cf204e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpf = fastparquet.ParquetFile(cpu_parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21aefe1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<Parquet File: {'name': '/tmp/int96_out_cpu.parquet/part-00000-060bb8b0-0127-49b9-8240-734bbc1fc42c-c000.snappy.parquet', 'columns': ['max_ts', 'max_big_ts'], 'partitions': [], 'rows': 1, 'row_groups': 1}>\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(fpf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8822fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '/tmp/int96_out_cpu.parquet/part-00000-060bb8b0-0127-49b9-8240-734bbc1fc42c-c000.snappy.parquet',\n",
       " 'columns': ['max_ts', 'max_big_ts'],\n",
       " 'partitions': [],\n",
       " 'rows': 1,\n",
       " 'row_groups': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpf.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07fb6268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_ts</th>\n",
       "      <th>max_big_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-14 07:38:45.418688</td>\n",
       "      <td>1854-06-04 08:29:37.999584768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      max_ts                    max_big_ts\n",
       "0 2023-07-14 07:38:45.418688 1854-06-04 08:29:37.999584768"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpf.head(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "24e1e440b58174d23459f334e6373b3f84a303d378405919ee47af328df355be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
