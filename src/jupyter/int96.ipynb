{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4146aabb-1c55-415e-b60c-23a1dcc1184a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Start jupyter-lab\n",
    "\n",
    "```bash\n",
    "jupyter-lab --notebook-dir=$HOME/gits/gerashegalov/rapids-shell/src/jupyter\n",
    "```\n",
    "or simply open in VS Code with Jupyter extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9c9ac",
   "metadata": {},
   "source": [
    "# Repro for [NVIDIA/spark-rapids#8625](https://github.com/NVIDIA/spark-rapids/issues/8625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4010da20-e354-4b10-b3ce-000344c59daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import findspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152499e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rapids_version = '23.08.0-SNAPSHOT'\n",
    "spark_version = '3.4.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab907e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'\n",
    "os.environ['TZ'] = 'UTC'\n",
    "\n",
    "home_dir = os.environ['HOME']\n",
    "work_dir = f\"{home_dir}/jupyter_run_dir\"\n",
    "spark_home = f\"{home_dir}/dist/spark-3.4.1-bin-hadoop3\"\n",
    "cores_per_exec = 1\n",
    "spark_master = f\"local[{cores_per_exec}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15dd6b39-9f91-4855-ab13-25989937df4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/14 14:48:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/14 14:48:25 WARN RapidsPluginUtils: RAPIDS Accelerator 23.08.0-SNAPSHOT using cudf 23.08.0-SNAPSHOT.\n",
      "23/07/14 14:48:25 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n",
      "23/07/14 14:48:25 WARN RapidsPluginUtils: spark.rapids.sql.explain is set to `ALL`. Set it to 'NONE' to suppress the diagnostics logging about the query placement on the GPU.\n"
     ]
    }
   ],
   "source": [
    "findspark.init(spark_home = spark_home)\n",
    "findspark.add_jars(f\"{home_dir}/gits/NVIDIA/spark-rapids/dist/target/rapids-4-spark_2.12-{rapids_version}-cuda11.jar\")\n",
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('RAPIDS PySpark Notebook') \\\n",
    "    .master(spark_master) \\\n",
    "    .config(map = {\n",
    "        'spark.plugins': 'com.nvidia.spark.SQLPlugin',\n",
    "        'spark.rapids.sql.explain': 'ALL', \n",
    "        'spark.sql.adaptive.enabled': False,\n",
    "    }) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd3a6125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(5000, 7, 14, 7, 38, 45, 418688)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = datetime.datetime(5000, 7, 14, 7, 38, 45, 418688)\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2438c36-df9b-4177-85da-b114a0d16f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/14 14:48:31 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  *Exec <ProjectExec> will run on GPU\n",
      "    *Expression <Alias> cast(ts_ntz#0 as string) AS ts_ntz#4 will run on GPU\n",
      "      *Expression <Cast> cast(ts_ntz#0 as string) will run on GPU\n",
      "    ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "      @Expression <AttributeReference> ts_ntz#0 could run on GPU\n",
      "\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|ts_ntz                    |\n",
      "+--------------------------+\n",
      "|5000-07-14 07:38:45.418688|\n",
      "+--------------------------+\n",
      "\n",
      "root\n",
      " |-- ts_ntz: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([(ts,),], 'ts_ntz timestamp').createOrReplaceTempView('df1')\n",
    "spark.sql(\"SELECT * FROM df1\").show(truncate = False)\n",
    "spark.sql(\"SELECT * FROM df1\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b65b29aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INT96'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.parquet.outputTimestampType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "260b6007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/14 14:48:32 WARN GpuOverrides: \n",
      "*Exec <DataWritingCommandExec> will run on GPU\n",
      "  *Output <InsertIntoHadoopFsRelationCommand> will run on GPU\n",
      "  *Exec <WriteFilesExec> will run on GPU\n",
      "    *Exec <HashAggregateExec> will run on GPU\n",
      "      *Expression <AggregateExpression> max(ts_ntz#0) will run on GPU\n",
      "        *Expression <Max> max(ts_ntz#0) will run on GPU\n",
      "      *Expression <Alias> max(ts_ntz#0)#9 AS max(ts_ntz)#10 will run on GPU\n",
      "      *Exec <ShuffleExchangeExec> will run on GPU\n",
      "        *Partitioning <SinglePartition$> will run on GPU\n",
      "        *Exec <HashAggregateExec> will run on GPU\n",
      "          *Expression <AggregateExpression> partial_max(ts_ntz#0) will run on GPU\n",
      "            *Expression <Max> max(ts_ntz#0) will run on GPU\n",
      "          ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "            @Expression <AttributeReference> ts_ntz#0 could run on GPU\n",
      "\n",
      "23/07/14 14:48:33 ERROR Utils: Aborting task\n",
      "java.lang.IllegalArgumentException: requirement failed: INT96 column contains one or more values that can overflow and will result in data corruption. Please set `spark.rapids.sql.format.parquet.writer.int96.enabled` to false so we can fallback on CPU for writing parquet but still take advantage of parquet read on the GPU.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7(GpuParquetFileFormat.scala:365)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7$adapted(GpuParquetFileFormat.scala:357)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:357)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:332)\n",
      "\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:228)\n",
      "\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:224)\n",
      "\tat com.nvidia.spark.rapids.ColumnCastUtil$.$anonfun$deepTransformView$1(ColumnCastUtil.scala:57)\n",
      "\tat com.nvidia.spark.rapids.Arm$.closeOnExcept(Arm.scala:130)\n",
      "\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransformView(ColumnCastUtil.scala:56)\n",
      "\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransform(ColumnCastUtil.scala:152)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter.deepTransformColumn(GpuParquetFileFormat.scala:332)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter.$anonfun$transform$1(GpuParquetFileFormat.scala:325)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1(implicits.scala:216)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1$adapted(implicits.scala:213)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.safeMap(implicits.scala:213)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$AutoCloseableProducingArray.safeMap(implicits.scala:263)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter.transform(GpuParquetFileFormat.scala:324)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4(ColumnarOutputWriter.scala:178)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4$adapted(ColumnarOutputWriter.scala:176)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$3(ColumnarOutputWriter.scala:176)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.withRestoreOnRetry(RmmRapidsRetryIterator.scala:229)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2(ColumnarOutputWriter.scala:176)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2$adapted(ColumnarOutputWriter.scala:170)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1(ColumnarOutputWriter.scala:170)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1$adapted(ColumnarOutputWriter.scala:164)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:431)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:542)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:468)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foreach(RmmRapidsRetryIterator.scala:487)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foldLeft(RmmRapidsRetryIterator.scala:487)\n",
      "\tat scala.collection.TraversableOnce.sum(TraversableOnce.scala:262)\n",
      "\tat scala.collection.TraversableOnce.sum$(TraversableOnce.scala:262)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.sum(RmmRapidsRetryIterator.scala:487)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatchWithRetry(ColumnarOutputWriter.scala:194)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatch(ColumnarOutputWriter.scala:153)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeAndClose(ColumnarOutputWriter.scala:115)\n",
      "\tat org.apache.spark.sql.rapids.GpuSingleDirectoryDataWriter.write(GpuFileFormatDataWriter.scala:245)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatDataWriter.writeWithIterator(GpuFileFormatDataWriter.scala:159)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.$anonfun$executeTask$1(GpuFileFormatWriter.scala:425)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:432)\n",
      "\tat org.apache.spark.sql.execution.datasources.GpuWriteFilesExec.$anonfun$doExecuteColumnarWrite$1(GpuWriteFiles.scala:138)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/07/14 14:48:33 ERROR GpuFileFormatWriter: Job job_202307141448327136221794454161928_0002 aborted.\n",
      "23/07/14 14:48:33 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:438)\n",
      "\tat org.apache.spark.sql.execution.datasources.GpuWriteFilesExec.$anonfun$doExecuteColumnarWrite$1(GpuWriteFiles.scala:138)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed: INT96 column contains one or more values that can overflow and will result in data corruption. Please set `spark.rapids.sql.format.parquet.writer.int96.enabled` to false so we can fallback on CPU for writing parquet but still take advantage of parquet read on the GPU.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7(GpuParquetFileFormat.scala:365)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7$adapted(GpuParquetFileFormat.scala:357)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:357)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:332)\n",
      "\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:228)\n",
      "\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:224)\n",
      "\tat com.nvidia.spark.rapids.ColumnCastUtil$.$anonfun$deepTransformView$1(ColumnCastUtil.scala:57)\n",
      "\tat com.nvidia.spark.rapids.Arm$.closeOnExcept(Arm.scala:130)\n",
      "\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransformView(ColumnCastUtil.scala:56)\n",
      "\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransform(ColumnCastUtil.scala:152)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter.deepTransformColumn(GpuParquetFileFormat.scala:332)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter.$anonfun$transform$1(GpuParquetFileFormat.scala:325)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1(implicits.scala:216)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1$adapted(implicits.scala:213)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.safeMap(implicits.scala:213)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$AutoCloseableProducingArray.safeMap(implicits.scala:263)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter.transform(GpuParquetFileFormat.scala:324)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4(ColumnarOutputWriter.scala:178)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4$adapted(ColumnarOutputWriter.scala:176)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$3(ColumnarOutputWriter.scala:176)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.withRestoreOnRetry(RmmRapidsRetryIterator.scala:229)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2(ColumnarOutputWriter.scala:176)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2$adapted(ColumnarOutputWriter.scala:170)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1(ColumnarOutputWriter.scala:170)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1$adapted(ColumnarOutputWriter.scala:164)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:431)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:542)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:468)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foreach(RmmRapidsRetryIterator.scala:487)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foldLeft(RmmRapidsRetryIterator.scala:487)\n",
      "\tat scala.collection.TraversableOnce.sum(TraversableOnce.scala:262)\n",
      "\tat scala.collection.TraversableOnce.sum$(TraversableOnce.scala:262)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.sum(RmmRapidsRetryIterator.scala:487)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatchWithRetry(ColumnarOutputWriter.scala:194)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatch(ColumnarOutputWriter.scala:153)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeAndClose(ColumnarOutputWriter.scala:115)\n",
      "\tat org.apache.spark.sql.rapids.GpuSingleDirectoryDataWriter.write(GpuFileFormatDataWriter.scala:245)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatDataWriter.writeWithIterator(GpuFileFormatDataWriter.scala:159)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.$anonfun$executeTask$1(GpuFileFormatWriter.scala:425)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:432)\n",
      "\t... 15 more\n",
      "23/07/14 14:48:33 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:438)\n",
      "\tat org.apache.spark.sql.execution.datasources.GpuWriteFilesExec.$anonfun$doExecuteColumnarWrite$1(GpuWriteFiles.scala:138)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed: INT96 column contains one or more values that can overflow and will result in data corruption. Please set `spark.rapids.sql.format.parquet.writer.int96.enabled` to false so we can fallback on CPU for writing parquet but still take advantage of parquet read on the GPU.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7(GpuParquetFileFormat.scala:365)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7$adapted(GpuParquetFileFormat.scala:357)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:357)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:332)\n",
      "\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:228)\n",
      "\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:224)\n",
      "\tat com.nvidia.spark.rapids.ColumnCastUtil$.$anonfun$deepTransformView$1(ColumnCastUtil.scala:57)\n",
      "\tat com.nvidia.spark.rapids.Arm$.closeOnExcept(Arm.scala:130)\n",
      "\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransformView(ColumnCastUtil.scala:56)\n",
      "\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransform(ColumnCastUtil.scala:152)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter.deepTransformColumn(GpuParquetFileFormat.scala:332)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter.$anonfun$transform$1(GpuParquetFileFormat.scala:325)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1(implicits.scala:216)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1$adapted(implicits.scala:213)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.safeMap(implicits.scala:213)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$AutoCloseableProducingArray.safeMap(implicits.scala:263)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter.transform(GpuParquetFileFormat.scala:324)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4(ColumnarOutputWriter.scala:178)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4$adapted(ColumnarOutputWriter.scala:176)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$3(ColumnarOutputWriter.scala:176)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.withRestoreOnRetry(RmmRapidsRetryIterator.scala:229)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2(ColumnarOutputWriter.scala:176)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2$adapted(ColumnarOutputWriter.scala:170)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1(ColumnarOutputWriter.scala:170)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1$adapted(ColumnarOutputWriter.scala:164)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:431)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:542)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:468)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foreach(RmmRapidsRetryIterator.scala:487)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foldLeft(RmmRapidsRetryIterator.scala:487)\n",
      "\tat scala.collection.TraversableOnce.sum(TraversableOnce.scala:262)\n",
      "\tat scala.collection.TraversableOnce.sum$(TraversableOnce.scala:262)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.sum(RmmRapidsRetryIterator.scala:487)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatchWithRetry(ColumnarOutputWriter.scala:194)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatch(ColumnarOutputWriter.scala:153)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeAndClose(ColumnarOutputWriter.scala:115)\n",
      "\tat org.apache.spark.sql.rapids.GpuSingleDirectoryDataWriter.write(GpuFileFormatDataWriter.scala:245)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatDataWriter.writeWithIterator(GpuFileFormatDataWriter.scala:159)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.$anonfun$executeTask$1(GpuFileFormatWriter.scala:425)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:432)\n",
      "\t... 15 more\n",
      "\n",
      "23/07/14 14:48:33 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job\n",
      "23/07/14 14:48:33 ERROR GpuFileFormatWriter: Aborting job 57826c93-9e9d-4135-84e2-a2ed16c735bd.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:438)\n",
      "\tat org.apache.spark.sql.execution.datasources.GpuWriteFilesExec.$anonfun$doExecuteColumnarWrite$1(GpuWriteFiles.scala:138)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed: INT96 column contains one or more values that can overflow and will result in data corruption. Please set `spark.rapids.sql.format.parquet.writer.int96.enabled` to false so we can fallback on CPU for writing parquet but still take advantage of parquet read on the GPU.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7(GpuParquetFileFormat.scala:365)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7$adapted(GpuParquetFileFormat.scala:357)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:357)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:332)\n",
      "\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:228)\n",
      "\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:224)\n",
      "\tat com.nvidia.spark.rapids.ColumnCastUtil$.$anonfun$deepTransformView$1(ColumnCastUtil.scala:57)\n",
      "\tat com.nvidia.spark.rapids.Arm$.closeOnExcept(Arm.scala:130)\n",
      "\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransformView(ColumnCastUtil.scala:56)\n",
      "\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransform(ColumnCastUtil.scala:152)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter.deepTransformColumn(GpuParquetFileFormat.scala:332)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter.$anonfun$transform$1(GpuParquetFileFormat.scala:325)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1(implicits.scala:216)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1$adapted(implicits.scala:213)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.safeMap(implicits.scala:213)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$AutoCloseableProducingArray.safeMap(implicits.scala:263)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter.transform(GpuParquetFileFormat.scala:324)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4(ColumnarOutputWriter.scala:178)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4$adapted(ColumnarOutputWriter.scala:176)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$3(ColumnarOutputWriter.scala:176)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.withRestoreOnRetry(RmmRapidsRetryIterator.scala:229)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2(ColumnarOutputWriter.scala:176)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2$adapted(ColumnarOutputWriter.scala:170)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1(ColumnarOutputWriter.scala:170)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1$adapted(ColumnarOutputWriter.scala:164)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:431)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:542)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:468)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foreach(RmmRapidsRetryIterator.scala:487)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foldLeft(RmmRapidsRetryIterator.scala:487)\n",
      "\tat scala.collection.TraversableOnce.sum(TraversableOnce.scala:262)\n",
      "\tat scala.collection.TraversableOnce.sum$(TraversableOnce.scala:262)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.sum(RmmRapidsRetryIterator.scala:487)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatchWithRetry(ColumnarOutputWriter.scala:194)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatch(ColumnarOutputWriter.scala:153)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeAndClose(ColumnarOutputWriter.scala:115)\n",
      "\tat org.apache.spark.sql.rapids.GpuSingleDirectoryDataWriter.write(GpuFileFormatDataWriter.scala:245)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatDataWriter.writeWithIterator(GpuFileFormatDataWriter.scala:159)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.$anonfun$executeTask$1(GpuFileFormatWriter.scala:425)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:432)\n",
      "\t... 15 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.$anonfun$executeWrite$6(GpuFileFormatWriter.scala:333)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.writeAndCommit(GpuFileFormatWriter.scala:297)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeWrite(GpuFileFormatWriter.scala:329)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.write(GpuFileFormatWriter.scala:199)\n",
      "\tat org.apache.spark.sql.rapids.GpuInsertIntoHadoopFsRelationCommand.runColumnar(GpuInsertIntoHadoopFsRelationCommand.scala:184)\n",
      "\tat com.nvidia.spark.rapids.GpuDataWritingCommandExec.sideEffectResult$lzycompute(GpuDataWritingCommandExec.scala:117)\n",
      "\tat com.nvidia.spark.rapids.GpuDataWritingCommandExec.sideEffectResult(GpuDataWritingCommandExec.scala:116)\n",
      "\tat com.nvidia.spark.rapids.GpuDataWritingCommandExec.internalDoExecuteColumnar(GpuDataWritingCommandExec.scala:140)\n",
      "\tat com.nvidia.spark.rapids.GpuExec.doExecuteColumnar(GpuExec.scala:349)\n",
      "\tat com.nvidia.spark.rapids.GpuExec.doExecuteColumnar$(GpuExec.scala:348)\n",
      "\tat com.nvidia.spark.rapids.GpuDataWritingCommandExec.doExecuteColumnar(GpuDataWritingCommandExec.scala:112)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:222)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:218)\n",
      "\tat com.nvidia.spark.rapids.GpuColumnarToRowExec.doExecute(GpuColumnarToRowExec.scala:333)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:445)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:438)\n",
      "\tat org.apache.spark.sql.execution.datasources.GpuWriteFilesExec.$anonfun$doExecuteColumnarWrite$1(GpuWriteFiles.scala:138)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed: INT96 column contains one or more values that can overflow and will result in data corruption. Please set `spark.rapids.sql.format.parquet.writer.int96.enabled` to false so we can fallback on CPU for writing parquet but still take advantage of parquet read on the GPU.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7(GpuParquetFileFormat.scala:365)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7$adapted(GpuParquetFileFormat.scala:357)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:357)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:332)\n",
      "\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:228)\n",
      "\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:224)\n",
      "\tat com.nvidia.spark.rapids.ColumnCastUtil$.$anonfun$deepTransformView$1(ColumnCastUtil.scala:57)\n",
      "\tat com.nvidia.spark.rapids.Arm$.closeOnExcept(Arm.scala:130)\n",
      "\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransformView(ColumnCastUtil.scala:56)\n",
      "\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransform(ColumnCastUtil.scala:152)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter.deepTransformColumn(GpuParquetFileFormat.scala:332)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter.$anonfun$transform$1(GpuParquetFileFormat.scala:325)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1(implicits.scala:216)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1$adapted(implicits.scala:213)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.safeMap(implicits.scala:213)\n",
      "\tat com.nvidia.spark.rapids.RapidsPluginImplicits$AutoCloseableProducingArray.safeMap(implicits.scala:263)\n",
      "\tat com.nvidia.spark.rapids.GpuParquetWriter.transform(GpuParquetFileFormat.scala:324)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4(ColumnarOutputWriter.scala:178)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4$adapted(ColumnarOutputWriter.scala:176)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$3(ColumnarOutputWriter.scala:176)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.withRestoreOnRetry(RmmRapidsRetryIterator.scala:229)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2(ColumnarOutputWriter.scala:176)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2$adapted(ColumnarOutputWriter.scala:170)\n",
      "\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1(ColumnarOutputWriter.scala:170)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1$adapted(ColumnarOutputWriter.scala:164)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:431)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:542)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:468)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foreach(RmmRapidsRetryIterator.scala:487)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foldLeft(RmmRapidsRetryIterator.scala:487)\n",
      "\tat scala.collection.TraversableOnce.sum(TraversableOnce.scala:262)\n",
      "\tat scala.collection.TraversableOnce.sum$(TraversableOnce.scala:262)\n",
      "\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.sum(RmmRapidsRetryIterator.scala:487)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatchWithRetry(ColumnarOutputWriter.scala:194)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatch(ColumnarOutputWriter.scala:153)\n",
      "\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeAndClose(ColumnarOutputWriter.scala:115)\n",
      "\tat org.apache.spark.sql.rapids.GpuSingleDirectoryDataWriter.write(GpuFileFormatDataWriter.scala:245)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatDataWriter.writeWithIterator(GpuFileFormatDataWriter.scala:159)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.$anonfun$executeTask$1(GpuFileFormatWriter.scala:425)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
      "\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:432)\n",
      "\t... 15 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o58.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.writeAndCommit(GpuFileFormatWriter.scala:314)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeWrite(GpuFileFormatWriter.scala:329)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.write(GpuFileFormatWriter.scala:199)\n\tat org.apache.spark.sql.rapids.GpuInsertIntoHadoopFsRelationCommand.runColumnar(GpuInsertIntoHadoopFsRelationCommand.scala:184)\n\tat com.nvidia.spark.rapids.GpuDataWritingCommandExec.sideEffectResult$lzycompute(GpuDataWritingCommandExec.scala:117)\n\tat com.nvidia.spark.rapids.GpuDataWritingCommandExec.sideEffectResult(GpuDataWritingCommandExec.scala:116)\n\tat com.nvidia.spark.rapids.GpuDataWritingCommandExec.internalDoExecuteColumnar(GpuDataWritingCommandExec.scala:140)\n\tat com.nvidia.spark.rapids.GpuExec.doExecuteColumnar(GpuExec.scala:349)\n\tat com.nvidia.spark.rapids.GpuExec.doExecuteColumnar$(GpuExec.scala:348)\n\tat com.nvidia.spark.rapids.GpuDataWritingCommandExec.doExecuteColumnar(GpuDataWritingCommandExec.scala:112)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:222)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:218)\n\tat com.nvidia.spark.rapids.GpuColumnarToRowExec.doExecute(GpuColumnarToRowExec.scala:333)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:445)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:438)\n\tat org.apache.spark.sql.execution.datasources.GpuWriteFilesExec.$anonfun$doExecuteColumnarWrite$1(GpuWriteFiles.scala:138)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: INT96 column contains one or more values that can overflow and will result in data corruption. Please set `spark.rapids.sql.format.parquet.writer.int96.enabled` to false so we can fallback on CPU for writing parquet but still take advantage of parquet read on the GPU.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7(GpuParquetFileFormat.scala:365)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7$adapted(GpuParquetFileFormat.scala:357)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:357)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:332)\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:228)\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:224)\n\tat com.nvidia.spark.rapids.ColumnCastUtil$.$anonfun$deepTransformView$1(ColumnCastUtil.scala:57)\n\tat com.nvidia.spark.rapids.Arm$.closeOnExcept(Arm.scala:130)\n\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransformView(ColumnCastUtil.scala:56)\n\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransform(ColumnCastUtil.scala:152)\n\tat com.nvidia.spark.rapids.GpuParquetWriter.deepTransformColumn(GpuParquetFileFormat.scala:332)\n\tat com.nvidia.spark.rapids.GpuParquetWriter.$anonfun$transform$1(GpuParquetFileFormat.scala:325)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1(implicits.scala:216)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1$adapted(implicits.scala:213)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.safeMap(implicits.scala:213)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$AutoCloseableProducingArray.safeMap(implicits.scala:263)\n\tat com.nvidia.spark.rapids.GpuParquetWriter.transform(GpuParquetFileFormat.scala:324)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4(ColumnarOutputWriter.scala:178)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4$adapted(ColumnarOutputWriter.scala:176)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$3(ColumnarOutputWriter.scala:176)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.withRestoreOnRetry(RmmRapidsRetryIterator.scala:229)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2(ColumnarOutputWriter.scala:176)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2$adapted(ColumnarOutputWriter.scala:170)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1(ColumnarOutputWriter.scala:170)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1$adapted(ColumnarOutputWriter.scala:164)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:431)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:542)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:468)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foreach(RmmRapidsRetryIterator.scala:487)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foldLeft(RmmRapidsRetryIterator.scala:487)\n\tat scala.collection.TraversableOnce.sum(TraversableOnce.scala:262)\n\tat scala.collection.TraversableOnce.sum$(TraversableOnce.scala:262)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.sum(RmmRapidsRetryIterator.scala:487)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatchWithRetry(ColumnarOutputWriter.scala:194)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatch(ColumnarOutputWriter.scala:153)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeAndClose(ColumnarOutputWriter.scala:115)\n\tat org.apache.spark.sql.rapids.GpuSingleDirectoryDataWriter.write(GpuFileFormatDataWriter.scala:245)\n\tat org.apache.spark.sql.rapids.GpuFileFormatDataWriter.writeWithIterator(GpuFileFormatDataWriter.scala:159)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.$anonfun$executeTask$1(GpuFileFormatWriter.scala:425)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:432)\n\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.$anonfun$executeWrite$6(GpuFileFormatWriter.scala:333)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.writeAndCommit(GpuFileFormatWriter.scala:297)\n\t... 60 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:438)\n\tat org.apache.spark.sql.execution.datasources.GpuWriteFilesExec.$anonfun$doExecuteColumnarWrite$1(GpuWriteFiles.scala:138)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: INT96 column contains one or more values that can overflow and will result in data corruption. Please set `spark.rapids.sql.format.parquet.writer.int96.enabled` to false so we can fallback on CPU for writing parquet but still take advantage of parquet read on the GPU.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7(GpuParquetFileFormat.scala:365)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7$adapted(GpuParquetFileFormat.scala:357)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:357)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:332)\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:228)\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:224)\n\tat com.nvidia.spark.rapids.ColumnCastUtil$.$anonfun$deepTransformView$1(ColumnCastUtil.scala:57)\n\tat com.nvidia.spark.rapids.Arm$.closeOnExcept(Arm.scala:130)\n\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransformView(ColumnCastUtil.scala:56)\n\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransform(ColumnCastUtil.scala:152)\n\tat com.nvidia.spark.rapids.GpuParquetWriter.deepTransformColumn(GpuParquetFileFormat.scala:332)\n\tat com.nvidia.spark.rapids.GpuParquetWriter.$anonfun$transform$1(GpuParquetFileFormat.scala:325)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1(implicits.scala:216)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1$adapted(implicits.scala:213)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.safeMap(implicits.scala:213)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$AutoCloseableProducingArray.safeMap(implicits.scala:263)\n\tat com.nvidia.spark.rapids.GpuParquetWriter.transform(GpuParquetFileFormat.scala:324)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4(ColumnarOutputWriter.scala:178)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4$adapted(ColumnarOutputWriter.scala:176)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$3(ColumnarOutputWriter.scala:176)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.withRestoreOnRetry(RmmRapidsRetryIterator.scala:229)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2(ColumnarOutputWriter.scala:176)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2$adapted(ColumnarOutputWriter.scala:170)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1(ColumnarOutputWriter.scala:170)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1$adapted(ColumnarOutputWriter.scala:164)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:431)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:542)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:468)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foreach(RmmRapidsRetryIterator.scala:487)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foldLeft(RmmRapidsRetryIterator.scala:487)\n\tat scala.collection.TraversableOnce.sum(TraversableOnce.scala:262)\n\tat scala.collection.TraversableOnce.sum$(TraversableOnce.scala:262)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.sum(RmmRapidsRetryIterator.scala:487)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatchWithRetry(ColumnarOutputWriter.scala:194)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatch(ColumnarOutputWriter.scala:153)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeAndClose(ColumnarOutputWriter.scala:115)\n\tat org.apache.spark.sql.rapids.GpuSingleDirectoryDataWriter.write(GpuFileFormatDataWriter.scala:245)\n\tat org.apache.spark.sql.rapids.GpuFileFormatDataWriter.writeWithIterator(GpuFileFormatDataWriter.scala:159)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.$anonfun$executeTask$1(GpuFileFormatWriter.scala:425)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:432)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark\u001b[39m.\u001b[39;49msql(\u001b[39m\"\u001b[39;49m\u001b[39mSELECT MAX(*) FROM df1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mmode(\u001b[39m'\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mparquet(\u001b[39m'\u001b[39;49m\u001b[39m/tmp/int96_out.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/dist/spark-3.4.1-bin-hadoop3/python/pyspark/sql/readwriter.py:1656\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m-> 1656\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[0;32m~/dist/spark-3.4.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/dist/spark-3.4.1-bin-hadoop3/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/dist/spark-3.4.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o58.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.writeAndCommit(GpuFileFormatWriter.scala:314)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeWrite(GpuFileFormatWriter.scala:329)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.write(GpuFileFormatWriter.scala:199)\n\tat org.apache.spark.sql.rapids.GpuInsertIntoHadoopFsRelationCommand.runColumnar(GpuInsertIntoHadoopFsRelationCommand.scala:184)\n\tat com.nvidia.spark.rapids.GpuDataWritingCommandExec.sideEffectResult$lzycompute(GpuDataWritingCommandExec.scala:117)\n\tat com.nvidia.spark.rapids.GpuDataWritingCommandExec.sideEffectResult(GpuDataWritingCommandExec.scala:116)\n\tat com.nvidia.spark.rapids.GpuDataWritingCommandExec.internalDoExecuteColumnar(GpuDataWritingCommandExec.scala:140)\n\tat com.nvidia.spark.rapids.GpuExec.doExecuteColumnar(GpuExec.scala:349)\n\tat com.nvidia.spark.rapids.GpuExec.doExecuteColumnar$(GpuExec.scala:348)\n\tat com.nvidia.spark.rapids.GpuDataWritingCommandExec.doExecuteColumnar(GpuDataWritingCommandExec.scala:112)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:222)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:218)\n\tat com.nvidia.spark.rapids.GpuColumnarToRowExec.doExecute(GpuColumnarToRowExec.scala:333)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:445)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:438)\n\tat org.apache.spark.sql.execution.datasources.GpuWriteFilesExec.$anonfun$doExecuteColumnarWrite$1(GpuWriteFiles.scala:138)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: INT96 column contains one or more values that can overflow and will result in data corruption. Please set `spark.rapids.sql.format.parquet.writer.int96.enabled` to false so we can fallback on CPU for writing parquet but still take advantage of parquet read on the GPU.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7(GpuParquetFileFormat.scala:365)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7$adapted(GpuParquetFileFormat.scala:357)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:357)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:332)\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:228)\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:224)\n\tat com.nvidia.spark.rapids.ColumnCastUtil$.$anonfun$deepTransformView$1(ColumnCastUtil.scala:57)\n\tat com.nvidia.spark.rapids.Arm$.closeOnExcept(Arm.scala:130)\n\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransformView(ColumnCastUtil.scala:56)\n\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransform(ColumnCastUtil.scala:152)\n\tat com.nvidia.spark.rapids.GpuParquetWriter.deepTransformColumn(GpuParquetFileFormat.scala:332)\n\tat com.nvidia.spark.rapids.GpuParquetWriter.$anonfun$transform$1(GpuParquetFileFormat.scala:325)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1(implicits.scala:216)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1$adapted(implicits.scala:213)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.safeMap(implicits.scala:213)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$AutoCloseableProducingArray.safeMap(implicits.scala:263)\n\tat com.nvidia.spark.rapids.GpuParquetWriter.transform(GpuParquetFileFormat.scala:324)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4(ColumnarOutputWriter.scala:178)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4$adapted(ColumnarOutputWriter.scala:176)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$3(ColumnarOutputWriter.scala:176)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.withRestoreOnRetry(RmmRapidsRetryIterator.scala:229)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2(ColumnarOutputWriter.scala:176)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2$adapted(ColumnarOutputWriter.scala:170)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1(ColumnarOutputWriter.scala:170)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1$adapted(ColumnarOutputWriter.scala:164)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:431)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:542)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:468)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foreach(RmmRapidsRetryIterator.scala:487)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foldLeft(RmmRapidsRetryIterator.scala:487)\n\tat scala.collection.TraversableOnce.sum(TraversableOnce.scala:262)\n\tat scala.collection.TraversableOnce.sum$(TraversableOnce.scala:262)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.sum(RmmRapidsRetryIterator.scala:487)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatchWithRetry(ColumnarOutputWriter.scala:194)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatch(ColumnarOutputWriter.scala:153)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeAndClose(ColumnarOutputWriter.scala:115)\n\tat org.apache.spark.sql.rapids.GpuSingleDirectoryDataWriter.write(GpuFileFormatDataWriter.scala:245)\n\tat org.apache.spark.sql.rapids.GpuFileFormatDataWriter.writeWithIterator(GpuFileFormatDataWriter.scala:159)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.$anonfun$executeTask$1(GpuFileFormatWriter.scala:425)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:432)\n\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.$anonfun$executeWrite$6(GpuFileFormatWriter.scala:333)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.writeAndCommit(GpuFileFormatWriter.scala:297)\n\t... 60 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:438)\n\tat org.apache.spark.sql.execution.datasources.GpuWriteFilesExec.$anonfun$doExecuteColumnarWrite$1(GpuWriteFiles.scala:138)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: INT96 column contains one or more values that can overflow and will result in data corruption. Please set `spark.rapids.sql.format.parquet.writer.int96.enabled` to false so we can fallback on CPU for writing parquet but still take advantage of parquet read on the GPU.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7(GpuParquetFileFormat.scala:365)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.$anonfun$applyOrElse$7$adapted(GpuParquetFileFormat.scala:357)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:357)\n\tat com.nvidia.spark.rapids.GpuParquetWriter$$anonfun$deepTransformColumn$1.applyOrElse(GpuParquetFileFormat.scala:332)\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:228)\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:224)\n\tat com.nvidia.spark.rapids.ColumnCastUtil$.$anonfun$deepTransformView$1(ColumnCastUtil.scala:57)\n\tat com.nvidia.spark.rapids.Arm$.closeOnExcept(Arm.scala:130)\n\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransformView(ColumnCastUtil.scala:56)\n\tat com.nvidia.spark.rapids.ColumnCastUtil$.deepTransform(ColumnCastUtil.scala:152)\n\tat com.nvidia.spark.rapids.GpuParquetWriter.deepTransformColumn(GpuParquetFileFormat.scala:332)\n\tat com.nvidia.spark.rapids.GpuParquetWriter.$anonfun$transform$1(GpuParquetFileFormat.scala:325)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1(implicits.scala:216)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.$anonfun$safeMap$1$adapted(implicits.scala:213)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$MapsSafely.safeMap(implicits.scala:213)\n\tat com.nvidia.spark.rapids.RapidsPluginImplicits$AutoCloseableProducingArray.safeMap(implicits.scala:263)\n\tat com.nvidia.spark.rapids.GpuParquetWriter.transform(GpuParquetFileFormat.scala:324)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4(ColumnarOutputWriter.scala:178)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$4$adapted(ColumnarOutputWriter.scala:176)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$3(ColumnarOutputWriter.scala:176)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$.withRestoreOnRetry(RmmRapidsRetryIterator.scala:229)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2(ColumnarOutputWriter.scala:176)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$2$adapted(ColumnarOutputWriter.scala:170)\n\tat com.nvidia.spark.rapids.Arm$.withResource(Arm.scala:29)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1(ColumnarOutputWriter.scala:170)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.$anonfun$writeBatchWithRetry$1$adapted(ColumnarOutputWriter.scala:164)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$AutoCloseableAttemptSpliterator.next(RmmRapidsRetryIterator.scala:431)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.next(RmmRapidsRetryIterator.scala:542)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryAutoCloseableIterator.next(RmmRapidsRetryIterator.scala:468)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foreach(RmmRapidsRetryIterator.scala:487)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.foldLeft(RmmRapidsRetryIterator.scala:487)\n\tat scala.collection.TraversableOnce.sum(TraversableOnce.scala:262)\n\tat scala.collection.TraversableOnce.sum$(TraversableOnce.scala:262)\n\tat com.nvidia.spark.rapids.RmmRapidsRetryIterator$RmmRapidsRetryIterator.sum(RmmRapidsRetryIterator.scala:487)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatchWithRetry(ColumnarOutputWriter.scala:194)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeBatch(ColumnarOutputWriter.scala:153)\n\tat com.nvidia.spark.rapids.ColumnarOutputWriter.writeAndClose(ColumnarOutputWriter.scala:115)\n\tat org.apache.spark.sql.rapids.GpuSingleDirectoryDataWriter.write(GpuFileFormatDataWriter.scala:245)\n\tat org.apache.spark.sql.rapids.GpuFileFormatDataWriter.writeWithIterator(GpuFileFormatDataWriter.scala:159)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.$anonfun$executeTask$1(GpuFileFormatWriter.scala:425)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.rapids.GpuFileFormatWriter$.executeTask(GpuFileFormatWriter.scala:432)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT MAX(*) FROM df1\").write.mode('overwrite').parquet('/tmp/int96_out.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "24e1e440b58174d23459f334e6373b3f84a303d378405919ee47af328df355be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
