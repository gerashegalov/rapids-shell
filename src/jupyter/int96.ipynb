{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4146aabb-1c55-415e-b60c-23a1dcc1184a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Start jupyter-lab\n",
    "\n",
    "```bash\n",
    "jupyter-lab --notebook-dir=$HOME/gits/gerashegalov/rapids-shell/src/jupyter\n",
    "```\n",
    "or simply open in VS Code with Jupyter extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9c9ac",
   "metadata": {},
   "source": [
    "# Repro for [NVIDIA/spark-rapids#8625](https://github.com/NVIDIA/spark-rapids/issues/8625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4010da20-e354-4b10-b3ce-000344c59daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import fastparquet\n",
    "import findspark\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72923f92",
   "metadata": {},
   "source": [
    "### Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152499e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_version = 'cuda11'\n",
    "hadoop_version = '3.2'\n",
    "java_version = '8'\n",
    "rapids_version = '23.10.0-SNAPSHOT'\n",
    "scala_version = '2.12'\n",
    "spark_version = '3.1.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84d10a",
   "metadata": {},
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1b741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME'] = f\"/usr/lib/jvm/java-{java_version}-openjdk-amd64\"\n",
    "os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'\n",
    "os.environ['TZ'] = 'UTC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab907e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.environ['HOME']\n",
    "work_dir = f\"{home_dir}/jupyter_run_dir\"\n",
    "m2_local_repo = f\"{home_dir}/.m2/repository\"\n",
    "groupId = \"com.nvidia\"\n",
    "artifactId = f\"rapids-4-spark_{scala_version}\"\n",
    "dist_jar = f\"{artifactId}-{rapids_version}-{cuda_version}.jar\"\n",
    "spark_home = f\"{home_dir}/dist/spark-{spark_version}-bin-hadoop{hadoop_version}\"\n",
    "# spark_home = f\"{home_dir}/gits/apache/spark\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29534b21",
   "metadata": {},
   "source": [
    "### Find Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1dfd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(spark_home = spark_home)\n",
    "findspark.add_jars(f\"{m2_local_repo}/com/nvidia/{artifactId}/{rapids_version}/{dist_jar}\")\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242319ae",
   "metadata": {},
   "source": [
    "### Configure Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15dd6b39-9f91-4855-ab13-25989937df4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for transport dt_socket at address: 5005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/29 19:25:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/29 19:25:31 WARN RapidsPluginUtils: RAPIDS Accelerator 23.10.0-SNAPSHOT using cudf 23.10.0-SNAPSHOT.\n",
      "23/07/29 19:25:31 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n",
      "23/07/29 19:25:31 WARN RapidsPluginUtils: spark.rapids.sql.explain is set to `ALL`. Set it to 'NONE' to suppress the diagnostics logging about the query placement on the GPU.\n"
     ]
    }
   ],
   "source": [
    "cores_per_exec = 1\n",
    "jdwp = '-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005'\n",
    "spark_master = f\"local[{cores_per_exec}]\"\n",
    "spark_builder = pyspark.sql.SparkSession.builder\n",
    "spark_builder.config('spark.app.name', 'RAPIDS PySpark Notebook')\n",
    "spark_builder.config('spark.driver.extraJavaOptions', f\"-Dai.rapids.cudf.preserve-dependencies=true {jdwp}\")\n",
    "spark_builder.config('spark.master', spark_master)\n",
    "spark_builder.config('spark.plugins', 'com.nvidia.spark.SQLPlugin')\n",
    "spark_builder.config('spark.rapids.sql.explain', 'ALL')\n",
    "\n",
    "spark = spark_builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31634b93",
   "metadata": {},
   "source": [
    "# Test Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd3a6125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(3023, 7, 14, 7, 38, 45, 418688),)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_tup = (\n",
    "    # datetime.datetime(2023, 7, 14, 7, 38, 45, 418688),\n",
    "    datetime.datetime(3023, 7, 14, 7, 38, 45, 418688),\n",
    ")\n",
    "ts_tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2438c36-df9b-4177-85da-b114a0d16f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- big_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/29 19:25:42 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  *Exec <ProjectExec> will run on GPU\n",
      "    *Expression <Alias> cast(big_ts#0 as string) AS big_ts#3 will run on GPU\n",
      "      *Expression <Cast> cast(big_ts#0 as string) will run on GPU\n",
      "    ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "      @Expression <AttributeReference> big_ts#0 could run on GPU\n",
      "\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|big_ts                    |\n",
      "+--------------------------+\n",
      "|3023-07-14 07:38:45.418688|\n",
      "+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([ts_tup,], 'big_ts timestamp')\n",
    "df.printSchema()\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffd76204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Chip(Enum):\n",
    "    CPU = 1\n",
    "    GPU = 2\n",
    "\n",
    "def test_path(chip, type_str):\n",
    "    return f\"/tmp/out_{chip}_{type_str}.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c142cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_case(chip, parquet_ts_type = 'INT96'):\n",
    "    print(f\"##### TEST chip={chip} parquet_ts_type={parquet_ts_type}\\n\")\n",
    "    spark.conf.set('spark.rapids.sql.enabled', chip == Chip.GPU)\n",
    "    spark.conf.set('spark.sql.parquet.outputTimestampType', parquet_ts_type)\n",
    "    path = test_path(chip, parquet_ts_type)\n",
    "    print(f\"IO to/from {path}\")\n",
    "    df.write.mode('overwrite').parquet(path)\n",
    "    spark.read.parquet(path).show(truncate = False)\n",
    "    parquet_file_path, = glob.glob(f\"{path}/*.parquet\")\n",
    "    pf = fastparquet.ParquetFile(parquet_file_path)\n",
    "    print(f\"fastparquet metadata {pf.fmd}\")\n",
    "    print(f\"fastparquet data: {pf.head(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c616d4",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65e99255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### TEST on_gpu=Chip.GPU parquet_ts_type=INT96\n",
      "\n",
      "IO to/from /tmp/out_Chip.GPU_INT96.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/29 19:25:43 WARN GpuOverrides: \n",
      "*Exec <DataWritingCommandExec> will run on GPU\n",
      "  *Output <InsertIntoHadoopFsRelationCommand> will run on GPU\n",
      "  ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "    @Expression <AttributeReference> big_ts#0 could run on GPU\n",
      "\n",
      "23/07/29 19:25:44 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  *Exec <ProjectExec> will run on GPU\n",
      "    *Expression <Alias> cast(big_ts#8 as string) AS big_ts#11 will run on GPU\n",
      "      *Expression <Cast> cast(big_ts#8 as string) will run on GPU\n",
      "    *Exec <FileSourceScanExec> will run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|big_ts                    |\n",
      "+--------------------------+\n",
      "|1854-06-04 08:29:37.999584|\n",
      "+--------------------------+\n",
      "\n",
      "fastparquet metadata column_orders:\n",
      "- TYPE_ORDER: {}\n",
      "created_by: null\n",
      "encryption_algorithm: null\n",
      "footer_signing_key_metadata: null\n",
      "key_value_metadata:\n",
      "- key: b'org.apache.spark.sql.parquet.row.metadata'\n",
      "  value: b'{\"type\":\"struct\",\"fields\":[{\"name\":\"big_ts\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}]}'\n",
      "- key: b'org.apache.spark.version'\n",
      "  value: b'3.1.1'\n",
      "num_rows: 1\n",
      "row_groups:\n",
      "- columns:\n",
      "  - column_index_length: null\n",
      "    column_index_offset: null\n",
      "    crypto_metadata: null\n",
      "    encrypted_column_metadata: null\n",
      "    file_offset: 0\n",
      "    file_path: null\n",
      "    meta_data:\n",
      "      bloom_filter_offset: null\n",
      "      codec: 0\n",
      "      data_page_offset: 4\n",
      "      dictionary_page_offset: null\n",
      "      encoding_stats: null\n",
      "      encodings:\n",
      "      - 0\n",
      "      - 3\n",
      "      index_page_offset: null\n",
      "      key_value_metadata: null\n",
      "      num_values: 1\n",
      "      path_in_schema:\n",
      "      - big_ts\n",
      "      statistics:\n",
      "        distinct_count: null\n",
      "        max: null\n",
      "        max_value: b'\\xc0N$\\xedD\\x1dv\\x00'\n",
      "        min: null\n",
      "        min_value: b'\\xc0N$\\xedD\\x1dv\\x00'\n",
      "        null_count: 0\n",
      "      total_compressed_size: 35\n",
      "      total_uncompressed_size: 35\n",
      "      type: 3\n",
      "    offset_index_length: null\n",
      "    offset_index_offset: null\n",
      "  file_offset: null\n",
      "  num_rows: 1\n",
      "  ordinal: null\n",
      "  sorting_columns: null\n",
      "  total_byte_size: 35\n",
      "  total_compressed_size: null\n",
      "schema:\n",
      "- converted_type: null\n",
      "  field_id: null\n",
      "  logicalType: null\n",
      "  name: schema\n",
      "  num_children: 1\n",
      "  precision: null\n",
      "  repetition_type: null\n",
      "  scale: null\n",
      "  type: null\n",
      "  type_length: null\n",
      "- converted_type: null\n",
      "  field_id: null\n",
      "  logicalType: null\n",
      "  name: big_ts\n",
      "  num_children: null\n",
      "  precision: null\n",
      "  repetition_type: 1\n",
      "  scale: null\n",
      "  type: 3\n",
      "  type_length: null\n",
      "version: 1\n",
      "\n",
      "fastparquet data:                          big_ts\n",
      "0 1854-06-04 08:29:37.999584768\n"
     ]
    }
   ],
   "source": [
    "test_case(chip = Chip.GPU, parquet_ts_type = 'INT96')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f23df4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### TEST on_gpu=Chip.GPU parquet_ts_type=TIMESTAMP_MICROS\n",
      "\n",
      "IO to/from /tmp/out_Chip.GPU_TIMESTAMP_MICROS.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/29 19:25:45 WARN GpuOverrides: \n",
      "*Exec <DataWritingCommandExec> will run on GPU\n",
      "  *Output <InsertIntoHadoopFsRelationCommand> will run on GPU\n",
      "  ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "    @Expression <AttributeReference> big_ts#0 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|big_ts                    |\n",
      "+--------------------------+\n",
      "|3023-07-14 07:38:45.418688|\n",
      "+--------------------------+\n",
      "\n",
      "fastparquet metadata column_orders:\n",
      "- TYPE_ORDER: {}\n",
      "created_by: null\n",
      "encryption_algorithm: null\n",
      "footer_signing_key_metadata: null\n",
      "key_value_metadata:\n",
      "- key: b'org.apache.spark.sql.parquet.row.metadata'\n",
      "  value: b'{\"type\":\"struct\",\"fields\":[{\"name\":\"big_ts\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}]}'\n",
      "- key: b'org.apache.spark.version'\n",
      "  value: b'3.1.1'\n",
      "num_rows: 1\n",
      "row_groups:\n",
      "- columns:\n",
      "  - column_index_length: null\n",
      "    column_index_offset: null\n",
      "    crypto_metadata: null\n",
      "    encrypted_column_metadata: null\n",
      "    file_offset: 0\n",
      "    file_path: null\n",
      "    meta_data:\n",
      "      bloom_filter_offset: null\n",
      "      codec: 0\n",
      "      data_page_offset: 4\n",
      "      dictionary_page_offset: null\n",
      "      encoding_stats: null\n",
      "      encodings:\n",
      "      - 0\n",
      "      - 3\n",
      "      index_page_offset: null\n",
      "      key_value_metadata: null\n",
      "      num_values: 1\n",
      "      path_in_schema:\n",
      "      - big_ts\n",
      "      statistics:\n",
      "        distinct_count: null\n",
      "        max: null\n",
      "        max_value: b'\\xc0N$\\xedD\\x1dv\\x00'\n",
      "        min: null\n",
      "        min_value: b'\\xc0N$\\xedD\\x1dv\\x00'\n",
      "        null_count: 0\n",
      "      total_compressed_size: 31\n",
      "      total_uncompressed_size: 31\n",
      "      type: 2\n",
      "    offset_index_length: null\n",
      "    offset_index_offset: null\n",
      "  file_offset: null\n",
      "  num_rows: 1\n",
      "  ordinal: null\n",
      "  sorting_columns: null\n",
      "  total_byte_size: 31\n",
      "  total_compressed_size: null\n",
      "schema:\n",
      "- converted_type: null\n",
      "  field_id: null\n",
      "  logicalType: null\n",
      "  name: schema\n",
      "  num_children: 1\n",
      "  precision: null\n",
      "  repetition_type: null\n",
      "  scale: null\n",
      "  type: null\n",
      "  type_length: null\n",
      "- converted_type: 10\n",
      "  field_id: null\n",
      "  logicalType: null\n",
      "  name: big_ts\n",
      "  num_children: null\n",
      "  precision: null\n",
      "  repetition_type: 1\n",
      "  scale: null\n",
      "  type: 2\n",
      "  type_length: null\n",
      "version: 1\n",
      "\n",
      "fastparquet data:                          big_ts\n",
      "0 1854-06-04 08:29:37.999584768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/29 19:25:45 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  *Exec <ProjectExec> will run on GPU\n",
      "    *Expression <Alias> cast(big_ts#16 as string) AS big_ts#19 will run on GPU\n",
      "      *Expression <Cast> cast(big_ts#16 as string) will run on GPU\n",
      "    *Exec <FileSourceScanExec> will run on GPU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_case(chip = Chip.GPU, parquet_ts_type = 'TIMESTAMP_MICROS') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e085af",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bcb78f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### TEST on_gpu=Chip.CPU parquet_ts_type=INT96\n",
      "\n",
      "IO to/from /tmp/out_Chip.CPU_INT96.parquet\n",
      "+--------------------------+\n",
      "|big_ts                    |\n",
      "+--------------------------+\n",
      "|3023-07-14 07:38:45.418688|\n",
      "+--------------------------+\n",
      "\n",
      "fastparquet metadata column_orders:\n",
      "- TYPE_ORDER: {}\n",
      "created_by: b'parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1)'\n",
      "encryption_algorithm: null\n",
      "footer_signing_key_metadata: null\n",
      "key_value_metadata:\n",
      "- key: b'org.apache.spark.version'\n",
      "  value: b'3.1.1'\n",
      "- key: b'org.apache.spark.sql.parquet.row.metadata'\n",
      "  value: b'{\"type\":\"struct\",\"fields\":[{\"name\":\"big_ts\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}]}'\n",
      "num_rows: 1\n",
      "row_groups:\n",
      "- columns:\n",
      "  - column_index_length: null\n",
      "    column_index_offset: null\n",
      "    crypto_metadata: null\n",
      "    encrypted_column_metadata: null\n",
      "    file_offset: 4\n",
      "    file_path: null\n",
      "    meta_data:\n",
      "      bloom_filter_offset: null\n",
      "      codec: 1\n",
      "      data_page_offset: 4\n",
      "      dictionary_page_offset: null\n",
      "      encoding_stats:\n",
      "      - count: 1\n",
      "        encoding: 2\n",
      "        page_type: 2\n",
      "      - count: 1\n",
      "        encoding: 2\n",
      "        page_type: 0\n",
      "      encodings:\n",
      "      - 3\n",
      "      - 4\n",
      "      - 2\n",
      "      index_page_offset: null\n",
      "      key_value_metadata: null\n",
      "      num_values: 1\n",
      "      path_in_schema:\n",
      "      - big_ts\n",
      "      statistics:\n",
      "        distinct_count: null\n",
      "        max: b'\\x00\\x9e\\xcd\\xc2\\x08\\x19\\x00\\x00\\xa6\\x1c+\\x00'\n",
      "        max_value: b'\\x00\\x9e\\xcd\\xc2\\x08\\x19\\x00\\x00\\xa6\\x1c+\\x00'\n",
      "        min: b'\\x00\\x9e\\xcd\\xc2\\x08\\x19\\x00\\x00\\xa6\\x1c+\\x00'\n",
      "        min_value: b'\\x00\\x9e\\xcd\\xc2\\x08\\x19\\x00\\x00\\xa6\\x1c+\\x00'\n",
      "        null_count: 0\n",
      "      total_compressed_size: 114\n",
      "      total_uncompressed_size: 110\n",
      "      type: 3\n",
      "    offset_index_length: null\n",
      "    offset_index_offset: null\n",
      "  file_offset: null\n",
      "  num_rows: 1\n",
      "  ordinal: null\n",
      "  sorting_columns: null\n",
      "  total_byte_size: 110\n",
      "  total_compressed_size: null\n",
      "schema:\n",
      "- converted_type: null\n",
      "  field_id: null\n",
      "  logicalType: null\n",
      "  name: spark_schema\n",
      "  num_children: 1\n",
      "  precision: null\n",
      "  repetition_type: null\n",
      "  scale: null\n",
      "  type: null\n",
      "  type_length: null\n",
      "- converted_type: null\n",
      "  field_id: null\n",
      "  logicalType: null\n",
      "  name: big_ts\n",
      "  num_children: null\n",
      "  precision: null\n",
      "  repetition_type: 1\n",
      "  scale: null\n",
      "  type: 3\n",
      "  type_length: null\n",
      "version: 1\n",
      "\n",
      "fastparquet data:                          big_ts\n",
      "0 1854-06-04 08:29:37.999584768\n"
     ]
    }
   ],
   "source": [
    "test_case(chip = Chip.CPU, parquet_ts_type = 'INT96')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d826a2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### TEST on_gpu=Chip.CPU parquet_ts_type=TIMESTAMP_MICROS\n",
      "\n",
      "IO to/from /tmp/out_Chip.CPU_TIMESTAMP_MICROS.parquet\n",
      "+--------------------------+\n",
      "|big_ts                    |\n",
      "+--------------------------+\n",
      "|3023-07-14 07:38:45.418688|\n",
      "+--------------------------+\n",
      "\n",
      "fastparquet metadata column_orders:\n",
      "- TYPE_ORDER: {}\n",
      "created_by: b'parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1)'\n",
      "encryption_algorithm: null\n",
      "footer_signing_key_metadata: null\n",
      "key_value_metadata:\n",
      "- key: b'org.apache.spark.version'\n",
      "  value: b'3.1.1'\n",
      "- key: b'org.apache.spark.sql.parquet.row.metadata'\n",
      "  value: b'{\"type\":\"struct\",\"fields\":[{\"name\":\"big_ts\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}]}'\n",
      "num_rows: 1\n",
      "row_groups:\n",
      "- columns:\n",
      "  - column_index_length: null\n",
      "    column_index_offset: null\n",
      "    crypto_metadata: null\n",
      "    encrypted_column_metadata: null\n",
      "    file_offset: 4\n",
      "    file_path: null\n",
      "    meta_data:\n",
      "      bloom_filter_offset: null\n",
      "      codec: 1\n",
      "      data_page_offset: 4\n",
      "      dictionary_page_offset: null\n",
      "      encoding_stats:\n",
      "      - count: 1\n",
      "        encoding: 0\n",
      "        page_type: 0\n",
      "      encodings:\n",
      "      - 3\n",
      "      - 0\n",
      "      - 4\n",
      "      index_page_offset: null\n",
      "      key_value_metadata: null\n",
      "      num_values: 1\n",
      "      path_in_schema:\n",
      "      - big_ts\n",
      "      statistics:\n",
      "        distinct_count: null\n",
      "        max: b'\\xc0N$\\xedD\\x1dv\\x00'\n",
      "        max_value: b'\\xc0N$\\xedD\\x1dv\\x00'\n",
      "        min: b'\\xc0N$\\xedD\\x1dv\\x00'\n",
      "        min_value: b'\\xc0N$\\xedD\\x1dv\\x00'\n",
      "        null_count: 0\n",
      "      total_compressed_size: 77\n",
      "      total_uncompressed_size: 75\n",
      "      type: 2\n",
      "    offset_index_length: null\n",
      "    offset_index_offset: null\n",
      "  file_offset: null\n",
      "  num_rows: 1\n",
      "  ordinal: null\n",
      "  sorting_columns: null\n",
      "  total_byte_size: 75\n",
      "  total_compressed_size: null\n",
      "schema:\n",
      "- converted_type: null\n",
      "  field_id: null\n",
      "  logicalType: null\n",
      "  name: spark_schema\n",
      "  num_children: 1\n",
      "  precision: null\n",
      "  repetition_type: null\n",
      "  scale: null\n",
      "  type: null\n",
      "  type_length: null\n",
      "- converted_type: 10\n",
      "  field_id: null\n",
      "  logicalType: null\n",
      "  name: big_ts\n",
      "  num_children: null\n",
      "  precision: null\n",
      "  repetition_type: 1\n",
      "  scale: null\n",
      "  type: 2\n",
      "  type_length: null\n",
      "version: 1\n",
      "\n",
      "fastparquet data:                          big_ts\n",
      "0 1854-06-04 08:29:37.999584768\n"
     ]
    }
   ],
   "source": [
    "test_case(chip = Chip.CPU, parquet_ts_type = 'TIMESTAMP_MICROS')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "24e1e440b58174d23459f334e6373b3f84a303d378405919ee47af328df355be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
